---
title: "The 10 Biggest ChatGPT-5 Problems & How to Fix Them"
video_id: "Gqnf5f1ITyo"
youtube_url: "https://www.youtube.com/watch?v=Gqnf5f1ITyo"
substack_url: null
publish_date: "2025-08-11"
duration: "21:58"
duration_seconds: 1318
view_count: 24491
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "Tutorial"
primary_topic: "AI Tools"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "OpenAI"
  people:
    []
  products:
    - "ChatGPT"
    - "Claude"
    - "Make"
    - "Opus"
    - "Artifacts"
  models:
    - "Claude Opus"
    - "SAM"
concepts:
  []
summary:
  - "# The 10 Biggest ChatGPT-5 Problems & How to Fix Them

So, the response to chat GPT5 has been a little bit like watching a mob with pitchforks come to the vampires castle"
keywords:
  - "ai-tools"
  - "artifacts"
  - "chatgpt"
  - "claude"
  - "coding"
  - "deep-dives"
  - "frameworks"
  - "make"
  - "openai"
  - "opus"
  - "prompting"
  - "tutorials"
  - "workflows"
---

# The 10 Biggest ChatGPT-5 Problems & How to Fix Them

So, the response to chat GPT5 has been a little bit like watching a mob with pitchforks come to the vampires castle. It's been wild to see people get so upset, so fed up with how the roll out was handled. And I don't just mean chartgate where famously, infamously chat GPT5 was rolled out with completely inaccurate charts in a live stream to hundreds of thousands of people. That's very fixable and OpenAI immediately fixed it. What I mean is that they chose to open AAI chose to end people's long-term relationships with their AI. And I don't just mean the sort of like vaguely creepy this is my AI girlfriend stuff. I mean they chose to end workflows. They chose to end professional engagements that people have with thinking partners. Everything you've built up with your AI with 40, with 03, with 03 Pro, it all went away within an hour or two after that video. and instead you got a brand new AI that was really like I've actually counted it up 10 different GPT5 models hiding inside the one GPT5 in a way that's predictable right the entire world spent a year telling open AI please stop giving us so many models in the dropdown but people still have really differing needs some people want really fast responses some people want a warm and empathetic model some people want really thoughtful responses some people want a lot of inference time. Some people want web search. Great. So, OpenAI gave us one model that was actually 10 models underneath with a router. And contrary to popular belief, this is not like a bunch of old models stitched together with a router. These are all new models and they're stitched in with a router. The problem is the router is cued to give OpenAI more room on their GPUs because their GPUs are melting with the kind of traffic that they get. And so the model router defaults to the dumber model for lack the non-reasoning model is the polite way to put it. What do we do with that? When do we need a non-reasoning fast model versus a model that's good? And how do we customize it? This video really focuses on top 10 named user complaints and concerns and what we can do to fix them in Chad GPT 5. I'm all about fixes, right? I'm all about being practical. This is the new default model for something like 700 million people whether we like it or not. And Sam Elman on his Reddit AMA where people came for him with forks. He was very clear. We're not going back. This is the model we have. I think it's a powerful model, but I think it needs like any model some working in some getting to know it. It's like going on a first date. I know that's going to sound weird and creepy, but stick with me, right? Andre Karpathy talked about these as stochastic people spirits. In this sense, you have to teach the stochastic people spirit what you need from it. And there are specific ways you can do that. And so I'm going to give you the top 10 issues that I've dug up on the internet about chat GPT5. And I'm going to tell you how I think they can be addressed. And we'll go through one at a time. Number one is router misouting. Now, part of that on day one was that one of their auto switch routers was actually offline. And so if you had day one issues but haven't had them since, that was probably what was going on. But if you still get shallow responses to complex questions because the router defaults to faster models, you want to get to a place where you can actually ask for thinking hard very clearly. I would recommend two things. One, just say think hard in the prompt. Let's not make this over complex. And two, go into the option to personalize your chat GPT and make it clear in the custom instructions what you want. as an example, default to deep analysis unless I say quick take and then go from there. But essentially, you're trying to push it and route it with the custom instructions as much as you can. Number two, chat versus API mismatch was also complained. So, chat GPT uses a routing system. API gives you direct model access. developers get a much different experience than the rest of us with chat GBT5 because developers can test a particular model in the sandbox, deploy it and get completely different behaviors. In this case, I think how Sam Alman is going to address it is he's going to start giving us more customizability and they've already rolled out in the last couple of hours a ability to see what model you're getting and what model's responding to you. And originally that wasn't the case. So, they're working hard to make this more visible in the chat. That's not really something we can fix with prompts. I promise to be honest with you about what you can fix and what you cannot. If you really care about controlling exactly which model you get every single time, you have only a couple of options. You can either go to the API or you can hit the drop down and you have you don't have 10 models of choices. You have if you're if you're a pro user, you have chat GPT5 pro and you have chat GBT5 thinking and you have chat GBT5. The options degrade from there down to plus and free users and so you have less and less choice and have to rely more on the prompting I gave you for router where you're prompting think hard. This is an issue that they are going to address with more customization. Number three, model drift and mismatch. So old workflows produce different outputs after migration to chat GPT5. That is somewhat inevitable. I would suggest if you've been running workflows in production that you I I hope that you have been keeping track of your prompts that you have been versioning your prompts and that when you have a new model that is responding differently with outputs because drift is inevitable with new models. Any new model would have produced drift that you then have the space to deliberately experiment with your prompt and adjust it to the right model. Now, if you're running a production pipeline, you get to select exactly which GPT5 model you want to use, and that gives you the flexibility to be much more controlled in your responses. If you're trying to run something through the chatbot flow, and a lot of people do, you are going to have to do more work to customize your prompt and more work to figure out how to route it to the right kind of model. And by the way, not every prompt needs a thinking model. Sometimes you want something quicker. I will say having worked with this model, you sometimes get more token output on the non-reasoning model because it's cheaper for them to produce those tokens. And so if you have like a thinking model produce an outline, you can have the non-thinking model do a lot of work for you in writing. Let's say you're writing a PRD. That might be a way to do it. And the non-thinking model, I know people come after it. This is just a little, you know, before we get to number four, this a little sidebar. The non-thinking model is remarkably smart for a non-thinking model. And it's also incredibly fast. And one of the things I noticed that is true about chat GPT5 that hasn't been true about previous models is that even if the non-thinking model isn't right the first time, it is so incredibly fast that you can get five or six responses back in the time it takes like Claude Opus 4 to do one response. It iterates into something that's really good in that time. And so in in a sense people are sort of sleeping on the value of speed there. Okay, let's go to number four. the long context illusion. So users have have assumed that if they stuff the the model with 200,000 tokens because they advertise, right? They they advertise they had a bigger token window that you'll get perfect recall. It's going to be good recall. It's going to be better recall than we've had in the past. It doesn't mean it's perfect. Even OpenAI's own evaluation admits something like 89% accuracy between 128 and 256,000 tokens. That's that's good. It's not perfect. There's still lost in the middle problems. you would still be wise to use U-shaped thinking in your prompting. Right? So, the mitigations are not new here. We've had challenges managing long context windows in the past. You want to anchor at the beginning with a strong prompt. You want to reiterate what you need to at the end. You can use techniques like um rhythmic reminders through the context window of what you're looking for. Claude showed us that with the system prompt. And so, there's a lot of techniques that we already know to manage this. And I think people just assume they didn't have to anymore. And as I emphasize over and over again, these are models within a lineage. They are getting better. But don't assume that everything you learned immediately breaks. Instead, assume that a lot of the techniques you've learned will evolve. And so in this case, it gets a little bit easier to recall the context. But still, those techniques work well. Let's move on to number five. If you ask for JSON and you just say, "Please return JSON." For whatever reason, chat GPT5 doesn't always do that. Sometimes it does. Sometimes it's invalid JSON objects. I would recommend that you ask specifically for structured outputs with JSON schema. And if you use JSON a lot, I would recommend getting into custom instructions with it and actually specifying what you're looking for. It's not that the system doesn't know it. It's that for whatever reason, each every model has flavors and tweaks. This model in early testing has had some issue with JSON objects. Now, that's not for every single one. This was specifically in some of the smaller versions of GPT5. GPT5 Mini had this issue. And so you might also switch to a different model. That's going to feel like a very coding specific tip, but we use these models for a lot of things and coding is one of them. Number six, tool action and how you handle tool action claims or calls. So the model will sometimes pretend to have called a tool or claim to have called a tool and done an action it didn't perform. 03 would do this too. An AI claims they reduce deception significantly. Anecdotally, that feels correct. It does do more of what I ask it to do, but the number is not zero. Whatever it is, I think they claimed it it's down to 2%. It's not zero. You need to be really clear about requiring the model in your prompt. This is whether you're API or chat. You need to be clear about requiring the model to show you a plan and then to show you the actions completed against the plan. In my initial notes, in my review that I published last Friday, I talked about the idea that this model does well with artifacts because artifacts are a way of proving that you can make a tool call and come back and do something. So if you need it to use Python, you don't just say use Python, you say, show me the Python greater that you made or show me the Python query you built. So you have to make it prove the artifact. I think that's something that is a little bit of a secret hack with shed GPT5 because we can't pick either the model directly in the chat nor can we define exactly the tool call in the chat. Those are ways to sort of force a tool call that get us what we want. And why does that matter? Because this model is designed to solve things with code. And sometimes you get solutions with code you wouldn't otherwise. My review on Friday called out that it is okay at making Gant charts just as an image. It is really good at making Gant charts with code and that that is a pattern that repeats for other problems. Number seven, thinking mode costs. Reasoning uses a lot of tokens and a lot of time and that is part of why it defaults to non-reasoning. And so we have people complaining and saying the thinking mode takes too long given what it's giving back. This is very much a preference. I am actually personally very okay with the model taking a few moments to think before it returns because I can feel the difference in the quality of response. If you don't want it to think that hard, this is actually the easiest one to solve. Pick regular chat GPT5 or if you're on a free uh or plus tier, it's going to default that way anyway. And just be happy and use that. And for a lot of people, honestly, that is probably good enough. By the way, the people who complain about non-reasoning are often complaining about either the quality of response, and we talked about going to thinking if you want it, or the lack of empathy in the non-thinking. And I have a I have a really easy fix for you on the empathy one. Go into your chat GPT personalization menu. You will have a style or a mode that you can use. And so, you can literally go in and you can say, I'm going to actually like read off to you all of the different options that you can check in the settings. So you go to settings, you go to customize chat GPT and you can select the personality. Personality is either default which is quick, clever and built to keep the conversation going which is absolutely true or cynic. I don't see many people asking for that one. Critical and sarcastic or robot efficient and blunt people complaining about it being robotic. It can be more robotic. Listener is thoughtful and supportive. I think that's the closest to the empathy people are looking for. Although OpenAI has said they're working to soften the overall profile of all of these personalities in response to customer feedback, the pitchforks or nerd exploratory and enthusiastic. So you can pick that personality. Now there are other custom instructions and this is what I've been saying when you customize chat GPT take advantage of that, right? Like I have it and I've introduced it. I've said I'm Nate. This is what I do and I've given it traits like for me I want it to think strategy first. I want it to be reflective. I want it to focus on high signal. I want it to push back on me. And so those are things that I've actually put into the customized instructions because they are what I want. You can do what you want with your custom instructions. And I think people are sort of sleeping on that as a way to handle chat GPT5 because that's what custom instructions are for. That's that's exactly what we should be doing. All right. So thinking mode costs absolutely fixable. In fact, I think that's one of the easier ones. Guard rail friction is interesting. And so there's there's certain cases where you are going to have appropriate questions for chat GPT 5 and it is a little bit more conservative around dual use content and there's particular risks especially around biohazards that it's super conservative about. Well, you probably want to think about how you use the model and how you ask for safe completions in those cases. That's a fairly limited like that's a very narrow wedge, but it's something that comes up if you were in biology, if you were in research. You may be asking for things that are entirely appropriate, but they tend to be right next to things that would be inappropriate to ask about. You are going to essentially need to evolve ways to talk to the model that prioritize safe completions in ways that are useful. Either that or you're going to honestly have to switch models for that one. Number nine, where it makes basic errors, the simplest fix is to require thinking mode. And the second simplest fix is to require verification and citations for factual claims. And you can actually lean on that in the custom instructions as well as a way of reinforcing that. Now, I will say I've emphasized customization and custom instructions a lot before we get to the 10th thing here. It will not override the system prompt in the chat if you are using the chat. One of the ways you know you can't overwrite it is you can demand that the custom instructions be verbose like super long- winded but OpenAI has to preserve their GPU capacity and so they are going to still impose token constraints and you can actually see it in the chain of thought. I've tried this if you ask it to be verbose and write long- winded stuff it is going to come back and it's going to say I have to respect OpenAI's token policies so I have to watch my output length. It literally shows you in the train of thought where it's adhering to the system prompt. And that's just good to know because essentially OpenAI has put some guard rails on that system prompt so that you can actually not break their GPU. I will do a separate video where I'll break down the system prompt that got leaked. I think it's super interesting. It's too long for this video. Uh but we'll get into it. It's a super interesting system prompt. So number 10, the silent fallback. So if you are on one of the lower plans, not one of the pro plans, if you hit something like 80 messages in 3 hours, it is going to silently downgrade the model and the quality can drop mid conversation. There will not be a warning. The only solution here is to monitor your usage and Chad GPT is working on a way to monitor that because they know that people want to see it. To use the API if you care about that as a developer or if you're not a developer to upgrade tiers if you really really care or to just go touch grass and take a walk. I wish there was a way to force it to sort of buy a prompt pack or or buy an upgrade pack for three hours or something. I think there'd be a lot of interest in that. That is not something that Jet GPT as a business has decided to do. All right. So, reviewing where we've been going through these 10. Number one, router misouting is a huge huge issue. You can fix this with prompts like think hard and also with custom instructions like default to deep analysis. Number two, chat and API being different because chat GPT uses a routing system in the chatbot and a API users can select the model. Well, honestly, the simplest fix there is to select the model. Or if you are using the chatbot on one of the higher tier plans, you can actually drop down and hit the model and like actually see like pro mode or whatever you want to test. You can also use the same number one fixes like think hard if you don't want to go if you don't have the option to go and hit the drop down. Number three, model retirement drift. So if a model was retired and your old workflows broke, what do you do? It's all about prompt versioning and making targeted upgrades and evaluating what happens. You should already have prompt versioning and you should already be evaluating. I've been preaching that for a long time. If you haven't been, this is where the bill comes due. Please start now. Number four, long context illusion. So, people assume that because of the advertisement-like quality of that OpenAI live stream that they could stuff in hundreds of thousands of tokens with perfect recall, but that's not what OpenAI actually claimed, and certainly not what I'm seeing in practice. You still need to use your good long context practices like U-shaped prompting where you emphasize at the beginning and the end what you're looking for and reiterating reiterating through the context window what you want. Context engineering still matters. I've been saying for a long time there is no way around good prompt engineering and good context engineering. That is a durable skill. Hey, it's a durable skill. Number five, JSON breaking. It feels like a narrow one, but it matters. We have had issues with smaller models with JSON breaking and not forming correct JSON. Either upgrade to a better model or be very clear that you want correctly formed JSON in your custom instructions and prompt for it very specifically with like you want structured outputs in complete JSON schema. Number six, tool action claims that are not true, like hallucinating tool calls. So this is where I called out that with this model in particular, getting artifacts matters. It's a way of forcing the tool call and forcing proof of tool call. Number seven, thinking mode cause people not wanting to use thinking mode when they don't want to. That one is actually one of the easiest. You just default to non-thinking. And if you really want to emphasize it, you can say don't think, act now, or get the faster answer, which is a little button that they added in chat GPT. Number eight, guardrail friction. This is another narrow one, but it's for the bio researcher folks out there, the folks using it for science and hard science. You may be asking queries that are close to dangerous requests or requests that OpenAI has deemed dangerous and it's using safe completions. You need to figure out how to narrowly tailor your request in the prompt or you need to switch models. Number nine, where it makes basic errors is probably using the non-reasoning model. So either upgrade to a better model or adjust your customization to require verification and citations for factual claims and really lean on that in the prompt as well. Then number 10, the silent mini fallback where like you use it 80 messages in 3 hours and it disappears. I wish that I wish this was fixable but like the open AI has to either up the limits which historically they tend to do or give you the ability to use a different model which they've talked about bringing 40 back or you're going to have to monitor your usage and maybe upgrade tiers. Now, there are people when I say at the beginning of this video, there are 10 things we can do to fix these issues, who are going to throw up their hands and say, "Why do I have to fix it? I was promised a magic thinking machine that would do the routing for me and do the thinking for me." I've seen that in my Tik Tok comments over the weekend. I was promised this and it didn't happen. Guys, there's no such thing as a free lunch. We spent an entire year asking chat GPT to take away all of the other models and give us one model that thinks well. And people will say, well, we didn't ask them to take away the models. But most people did. They said they didn't want the model drop down. If you don't want the model drop down, you want one model. Something has to give. And so now we have one model in the drop down or maybe like a couple flavors of the same model in the drop down depending on your plan. And we have to decide what to do with it. And there is no way you can make a transition that big and not have some teething problems and not have some issues with roll out and not have some issues with how we learn to use it. The idea that the intelligence from the sky, the magic rocks that think are going to magically be able to in a new model roll out understand exactly what you want in your vague English is not ever something that you should have anticipated to be blunt. It just isn't. Prompting is a durable skill. Understanding how models work is a durable skill. And increasingly being able to adjust and evolve your workflows with a new model is a durable skill. That's not going to go away. I'm going to keep exploring how chat GPT5 works because this is a model that is incredibly important in the world right now because it's now the only model that hundreds of millions of people are using every week and it is a complex model to use. My early impressions are that this takes more effort and more thinking and more deliberate intention to use really really well. even if the default feels kind of smart to some people. So the default may feel cold, but it can feel kind of smart enough to some people and I've seen that in my comments as well. If you want to use it for extraordinary work, which this model is capable of, I've tested it. It does incredible work. And I will do some more demos later this week that sort of show that you need to be ready to put in extra work versus what you would have had to do with 03 or with 03 Pro or with Cloud4 Opus. And you might be like, is it worth it? Is the extra work worth it? The answer is yes. I have seen this model do analysis that I haven't seen any other model complete successfully. It can oneshot or fewot coding examples for for software that you can use around the office that I haven't seen anything else do quite as successfully. It is worth the effort, but it is work. So, I hope this review of 10 common issues across chat GPT5 has been helpful. We continue our exploration of this new model we're all living with now. Uh, let me know what you think in the comments. There'll be no other issues I can address.
