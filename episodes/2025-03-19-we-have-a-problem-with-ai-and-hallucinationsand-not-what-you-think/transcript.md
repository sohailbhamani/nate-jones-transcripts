---
title: "We have a problem with AI and hallucinations—and not what you think"
video_id: "0IxUJJCBkPI"
youtube_url: "https://www.youtube.com/watch?v=0IxUJJCBkPI"
substack_url: null
publish_date: "2025-03-19"
duration: "9:17"
duration_seconds: 557
view_count: 6131
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Executives"
entities:
  companies:
    - "Anthropic"
    - "Google"
    - "YouTube"
  people:
    []
  products:
    - "ChatGPT"
    - "Claude"
    - "Gemini"
    - "Make"
  models:
    - "Gemini"
concepts:
  []
summary:
  - "5% and 15% and by the way I'm not making that up that's roughly where ChatGpt 4"
keywords:
  - "ai-news"
  - "ai-tools"
  - "anthropic"
  - "chatgpt"
  - "claude"
  - "frameworks"
  - "gemini"
  - "google"
  - "leadership"
  - "make"
  - "openai"
  - "prompting"
  - "youtube"
---

# We have a problem with AI and hallucinations—and not what you think

i did not want to do this we are going to talk about hallucinations and the reason we're going to talk about hallucinations is because I can't get people to stop talking to me about hallucinations so here we are we're doing it uh look at the end of the day the fact that chat GPT was released when it was at the capability level it was released at means that we have a massive overhang of credibility that we have to make up chat GPT is much more credible than people believe it is claude is much more credible than people believe it is it's not really just about chat GPT but for the people who care about this it's always Chad GPT because that's the language model they know but Gemini same deal what I'm trying to say is when chat GPT was released back in 2022 there were enough high-profile hallucinations that people misunderstood what AI can actually do and chocked it up to a bunch of lies and I still hear that all the time and I am just it's like every every day I hear this and I'm just dealing with it i'm just going to talk about it what I want to say is that we have a different bar for AI than we have for humans for humans if I had an unreliable researcher frankly a human researcher who was an intern and that intern took a week to prepare me a 40-page report and if that intern made three mistakes in that 40-page report I would say great uh and I would love to use that report in whatever I'm working on if an AI comes back in 30 minutes with a 40page report and it makes three mistakes we say it's it's not good enough it needs to be perfect why it's already cut the time by a 100x why does it need to be perfect why does it need to be more perfect than people now there's other reasons to say that herist you know hallucinations are not that big a deal um but I think that's the most compelling one to me because if you want AI to do useful work then you just have to believe that the work it can do is more useful than the time it takes to check for hallucinations and we are well past that bar does that mean that hallucinations don't matter does that mean a lawyer should not be checking their case by case citations if they're using AI does that mean a doctor shouldn't be checking the medical reasoning of an AI obviously not obviously we should be checking and we should be working to reduce hallucinations great but the fact that we are at a point now where it can clearly and obviously do useful work means that AI has crossed the event horizon it is no longer just a play thing is something we can do work with and I think unfortunately that credibility overhang is biting this industry in the butt because at the end of the day most people who are not sitting in this YouTube circle if I talk to them about AI hallucinations are the first thing out of their mouth it's the first thing they talk about hey what about hallucinations i heard they make stuff up i heard it lies honestly it lies less than the average human does at this point most of them the hallucination rate which by the way it's really hard to measure hallucination rate i looked into this i wrote a Substack about this if you want to check it out if you don't I don't care it's a good read though um and it goes deep in on what hallucinations are and one of the things that I think is really interesting is that what we call the hallucination rate varies by a factor of 10 depending on the task you give it the same model can come in at 1 and a.5% and 15% and by the way I'm not making that up that's roughly where ChatGpt 4.5 comes in depending on which hallucination measure you use context really matters the kind of task you give it really matters one of the reasons why I don't worry about hallucinations personally is because I don't give AI a situation where it is likely to make up hallucinations and then blame it i figure that's mismanaging my employee like why would I do that i don't ask AI to do things that are virtually impossible unless it imagines or hallucinates or confabulates information because that's useless why would I do that it's such a powerful tool for what it can do well why not specify your sources where you want it to go look why not be careful in my prompting and be really clear and structured because it does well when I do that that's just easier for me so a lot of these things that actually reduce hallucinations turns out that they're just best practice for working with AI i don't know seems like we should follow best practice and so to me like our open AI our anthropic are they working on this sure is Deep Mind at Google working on this absolutely does that mean that we're going to have 100% no hallucination models next year i guarantee you we will not and I also just about guarantee you it won't matter it won't matter for real work it's going to matter enormously for public perception because we are trained to assume that computers must be perfect because everything we've had in computers for 100 years well not 100 years call it 60 years has been deterministic computing it has been programs that if a plus b equals c then whatever right like it's all mathematics it's algorithmic everything is determined in the program when it runs and so we can expect perfection and all of our movies say the same thing none of us are ready for an AI where we taught the rocks to think and they turn out to be poetic dreamers we're just not ready for that and the fact that the the like AI doesn't inherently have a factual world model the fact that we can talk about a 1.5% error rate in certain hallucination tests for chat GPT 4.5 is a freaking miracle i I am astonished these things they dream they come up with probabilistic tokens that they think match what you're looking for they have no factual world model underneath it's amazing they get anything right at all it's kind of incredible and so within that world yeah I do think we need to baseline on humans more i do think we need to take seriously the fact that they do work and I think that we need to come up with better answers as an industry for people who say all it does is lie all it does is make stuff up it's re and by the way the people who do that tend to be quite unreliable narrators themselves i have never heard that kind of aggressive contrarian take from someone who isn't to some degree personally threatened by AI and needing to denigrate it so there is absolutely a leading edge of change here people who are worried about their jobs people who are worried about what will happen to their work are going to be more likely to denigrate AI and do I have a study for that i will admit frankly I don't that is based on me having conversations with hundreds of people it's just something I've observed so where does that leave us at the end of the day AI is going to get to a point in fact arguably is already crossing the line where it is more reliable in most fields than most humans at which point we should stop worrying so much about hallucination for AI and logically worry about hallucination for ourselves and we're not and the reason why we're not is pretty simple it's the same reason why Whimo vehicles are not more popular even though they're vastly safer it's the same reason why we haven't outlawed human driving in the US even though statistically speaking in US testing automated driving is already so much safer it costs lives to keep human drivers on the road and I say the US because that's where it's been tested it's probably true everywhere else in the world too we are a stubborn stubborn race we are a stubborn species we do not easily give up on something we think is true we think humans should drive i do not see that disappearing anytime soon even though that kills people we think AI hallucinates i don't think that belief is disappearing even though it is demonstrabably easily obviously proved to be an unhelpful belief but we have to try we have to try and explain to people what really matters here we have to do our best to educate and this is a challenge for all of us in the industry and I just I got so tired of hearing about hallucinations i just I wrote a giant Substack on it i did this like we've got to be able to
