---
title: "Feb 5: Google Releases Three New Flash Models—But Does It Matter?"
video_id: "DaDxAwF_JSI"
youtube_url: "https://www.youtube.com/watch?v=DaDxAwF_JSI"
substack_url: null
publish_date: "2025-02-05"
duration: "4:04"
duration_seconds: 244
view_count: 3442
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Product Managers"
entities:
  companies:
    - "Google"
  people:
    []
  products:
    - "Claude"
    - "Gemini"
  models:
    - "Gemini"
    - "Gemini 2"
concepts:
  []
summary:
  - "# Feb 5: Google Releases Three New Flash Models—But Does It Matter"
keywords:
  - "ai-news"
  - "ai-tools"
  - "anthropic"
  - "career"
  - "claude"
  - "coding"
  - "frameworks"
  - "gemini"
  - "google"
  - "product-management"
  - "prompting"
---

# Feb 5: Google Releases Three New Flash Models—But Does It Matter?

I have a very simple question today is a Google release day today Gemini is coming out are we all chat GPT pilled or is Gemini actually not as useful as it looks like on paper because I'm going to go through I'm going to tell you about the Gemini 2.0 model release right like there's multiple things that came out if you think that open AI had a problem with naming wait till you see how Google is doing naming first they're introducing Google Gemini 2.0 flash it's highfrequency model it's usable at scale it has a context window of a million tokens and it's now generally available it tests very well anecdotally I will tell you that developers by and large most of the developers I talk to do not think it is as good as Claude even though the context window is much larger then they're also introducing 2.0 pro experiment mental our best model yet for coding performance they say um and again it tests very well now it has a context window of 2 million tokens I believe it's available in vertex now and finally they're introducing 2.0 flashlight uh and so they're trying to get something that is very fast very cheap to produce and able to work with you in Google AI studio and vertex AI I'm going to link to all of this you don't have to remember all of it you'll see the test results here's the larger Point those are three different models I can't tell you even more than open AI I cannot tell you what is the meaningful difference between those models and even Google when they're writing about it it isn't clear they talk about how good 2.0 flash is for developers and then they say but 2.0 pro experimental even better for developers okay maybe it is but you got to get clear on the product and package packaging and I'm not saying this because Google's alone on this a lot of the model makers are struggling with the packaging here it's just coming up with Google and it is part of what is prompting me to ask is the packaging the issue for Google or is it the model itself that isn't working and what's interesting is with open AI everyone agrees the packaging is the issue like the packaging is clearly a problem there's also been a lot of push back since deep seek on the fact that it's not open source but with Google yes the packaging is the issue but in addition I hear a lot of anecdotal evidence and I've seen it myself when I'm using it that these models don't perform in real world query scenarios as well as they test and so you can ask them something and I've actually included Gemini in some of my asks like when I've been doing tests on queries Gemini is one of the models I go to and what I see is that it's not as thoughtful it tends to infer less and infer more shallowly and I've been using 2.0 and I I don't know what to say it's very difficult to assess real world performance of models right now this is not a Google only challenge we need a better set of evals uh or evaluations that help us to assess models more clearly so that's where I am I think my question to you is is Google actually as good as it tests is Google not performing in real world scenarios as well as they claim and if that's the case is it worth following Google or is is this like enough of a string of ships where we think Google is actually having difficulty shipping core models even if they've done a great job on other things like I would argue with you that notebook LM is a great product straight up they've done a phenomenal job there is that where we're seeing progress from Google is in the UI and some of these side products and not in the core models that would be a really odd scenario what do you think
