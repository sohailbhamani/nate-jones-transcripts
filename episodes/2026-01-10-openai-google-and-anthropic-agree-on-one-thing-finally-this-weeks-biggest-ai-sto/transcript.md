---
title: "OpenAI, Google, and Anthropic Agree on One Thing (Finally) - This Week's Biggest AI Stories"
video_id: "TTMOSR-nKjg"
youtube_url: "https://www.youtube.com/watch?v=TTMOSR-nKjg"
publish_date: "2026-01-10"
duration: "12:42"
duration_seconds: 762
view_count: 70613
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  - "AI infrastructure"
  - "AI agents"
  - "NVIDIA Vera Rubin"
  - "OpenAI"
  - "Anthropic"
  - "AI strategy"
  - "enterprise AI"
  - "MCP protocol"
  - "prompt injection"
  - "AI security"
  - "power constraints"
  - "Meta Manus"
  - "AMD chips"
  - "Cursor Graphite"
  - "AI deployment"
  - "future of work"
  - "AI compute"
  - "grid modernization"



# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "OpenAI"
    - "Anthropic"
    - "Google"
    - "Meta"
    - "Microsoft"
    - "Nvidia"
    - "Cursor"
    - "AMD"
  people:
    - "Greg Brockman"
  products:
    - "Cursor"
    - "Make"
    - "MCP"
    - "Model Context Protocol"
    - "Atlas"
    - "Operator"
    - "Projects"
  models:
    []
concepts:
  []
summary:
  - "# OpenAI, Google, and Anthropic Agree on One Thing (Finally) - This Week's Biggest AI Stories

All right, if you took time out over the holidays, good for you"
keywords:
  - "ai-agents"
  - "ai-strategy"
  - "ai-tools"
  - "amd"
  - "anthropic"
  - "atlas"
  - "cursor"
  - "frameworks"
  - "google"
  - "leadership"
  - "make"
  - "mcp"
  - "meta"
  - "microsoft"
  - "model-context-protocol"
  - "nvidia"
  - "openai"
  - "operator"
  - "product-management"
  - "projects"
  - "workflows"
---

# OpenAI, Google, and Anthropic Agree on One Thing (Finally) - This Week's Biggest AI Stories

All right, if you took time out over the holidays, good for you. I took a little bit of time, too. But AI did not slow down. I'm going to give you the 10 stories that matter for 2026. And we're going to jump through them as quick as you can so you feel caught up and you know what is going to shape how we build this coming year. Number one, Nvidia's Vera Rubin platform, which was announced at CES on January 5th. Jensen Wong took the stage and he made something super clear. Nvidia is not a GPU company anymore. It is a platform company and Vera Rubin is building the factory of the future. This is not just a chip. It is a full six component stack. It has the Vera CPU, the Reuben GPU, the NVLink 6, the Connect X9 Super NIC, the Bluefield 4 DPU, and the Spectrum 6 Ethernet. It's designed to optimize for the extremely large context lengths we are going to give our next models, 10 million token context windows. And it's designed to do that at speed and cheaply. And what matters is that Nvidia is trying to own the definition of the AI factory of the future because the competition in 2026 is about real workloads under pressure. We are demand constrained right now. We have more demand that we can serve in AI and Nvidia is trying to say we're thinking about the whole system and we're installing it. If they succeed, it looks like faster, cheaper models for all of us, which looks like ambient AI everywhere by the second half of 2026. Story number two, Meta acquires Manis. This was back on December 29th. Reuters reporting it. Everybody's reporting it. $2 billion valuation is what's being quoted. I've heard between$2 and $3 billion. Why did Meta pay for this? Meta paid for this because building Agentic harnesses is hard to do in practice. And Manis is one of the few teams at an independent player who has delivered a fullyfeatured Agentic harness that can finish work at scale. That's what Manis was known for. It's an autonomous agent that operates from the browser that can finish work at scale. Zuck is hoping to tie that into his model ecosystem. He's hoping to tie it into internal tooling. My guess is he's hoping to tie it into ad builders. We will have to see. In the meantime, Manis continues to operate as a separate player in the space, but watch for data policy changes. I suspect they are coming. Story number three, AMD's CES Counter Punch. January 6th. Lisa Sue unveiled two new chips at CES. the MI455 focused on rack scale server deployments and the M1440X which AMD is explicitly positioning as an enterprisefriendly hardware for traditional business infrastructure. So that framing is super deliberate, right? So this is a chip that is designed to fit into your existing data center as an enterprise and it's not just designed to serve hyperscaler moonshots which is where Nvidia is focused. And of course the optics were interesting because who was on stage with Lisa? Open AI's Greg Brockman. AMD wants everybody to know they have a credible supplier at the frontier and they're not just a second source in the industry. So the AI stack is starting to split. That's the strategic read here. Demand is so high that we are seeing a multi-player future at the chip level. In 2026, you're going to see more enterprises that prefer architectures that allow hybrid deployments. something like a cloud that bursts for peak demand, on-prem inference for data residency. And AMD is positioning their chipsets as saying, we are the chip for the middle world. We are the chip for regulated industries, for enterprises with sovereignty concerns. We are a credible alternative to Nvidia. You don't have to only use chips that are designed for frontier training if you want to serve enterprise workloads. That's the AMD play. And I think that demand is exploding fast enough on the compute front that they are going to do very well out of that play without really taking away from Nvidia's growth story. Story number four, Microsoft is partnering with MISO on grid modernization. That was back on January 6th. This one flew under the radar for most people. I'm not surprised if you haven't heard about it, but it's significant. Microsoft announced a partnership with the Midcontinent Independent System Operator to modernize the Midwest Power System. The integration focuses on weather disruption prediction on transmission planning and operational efficiency as electricity demand from data centers surges. This is the power is now the compute constraint story. We have been talking about the idea that gigawatts and gigawatt stories are driving AI. Well, here we have it. Hyperscalers aren't just customers from the grid. They're becoming grid stakeholders. grid planning timelines, interconnection cues, those are now strategic dependencies for AI road mapaps to serve the demand that we're seeing. And so we should expect in 2026 a new competitive access around grid advantage. So sites, deals, partnerships that unlock power faster are going to become just as valuable for model makers as model performance itself. And utilities are going to start demanding something back. They're going to demand flexibility. They're going to demand forecasting from the model makers. They're going to demand a degree of demand response from them. And that's going to push data centers toward becoming a controllable load rather than a pure always on load. And companies that figure out how to be good grid citizens are going to have deployment advantages that their competitors can't easily replicate because they're going to get access to power. And so there's a little bit of the industry growing up and becoming a good citizen on the grid that we're starting to see here. Story number five is also about power. The bring your own power fight is heating up. The Wall Street Journal has been covering a growing clash between utilities, grid operators, and data center developers. PJM and other regional operators in the United States are proposing stop gaps, basically requiring data centers to bring their own power or disconnect during peak demand because grid upgrades are lagging behind. Texas has already passed legislation enabling disconnection during shortages and other regions are exploring similar opport. Meanwhile, the FERC has been shaping rules around large load interconnections and colllocations and has directed PJM to implement clearer rules for AIdriven large loads that are colllocated with generators because well, basically they're framing this as a reliability and a consumer cost issue. This is really the beginning of policy defined AI scale. If if power becomes something that is conditional, it's going to change everything because data centers have to get built and they have to get priced and you need to see reliability of power to make that happen. And so if reliability comes into question, which is really the story here as PJM and regional operators fight back, then we have to talk about other ways of bringing AI to the table. We may get into a conversation around AI load shaping, right? software and contracts that let operators commit to shedding 15 to 30% of load in emergencies without breaking their SLAs's. That's not on the table today. It may have to be. And at the same time, as we just saw in story number four, savvy players in the grid that act like good citizens may be able to secure power contracts that enable them to stay always on and deliver higher quality SLAs's and thus earn more business and soak up more of that demand and win in this race, which is really being defined as cost per token, gigawatt cost per token. Story number six, moving on from power, MCP is joining the Linux Foundation. So, Enthropic announced they're donating the model context protocol into the Linux Foundation's new Agentic AI foundation, and that foundation will be home to a bunch of foundational Aentic tech with initial projects including MCPs, the goose framework from Block and OpenAI's Agent Stopmarkdown. This is protocol de-risisking. This is good for the industry. Enterprises don't want their agent tooling pinned into one vendor's roadmap. It's not good for anybody. So putting MCP under a neutral umbrella is a great step toward building interoperable tool use and really enabling AI to become commodity infrastructure. It's exactly the precondition we need for a real middleware market to start to emerge. So MCP is not going to be just a nice standard in 2026. It is going to become a surface where we expect everything to operate. Think uh tool servers getting signed, permissioning frameworks, audit loops, providence tracking. The winner isn't going to be who implements MCP. It's who can make MCP safe and operable in regulated environments now that it is in a foundation framework. I think we're going to see a lot of blossoming and maturity in the MCP ecosystem this year. Speaking of which, Google launched MCP servers on December 10th. Google announced fully managed remote MCP servers as an official layer across Google and Google Cloud services. So developers can now point agents at enterprise ready endpoints rather than handrolling connectors for every single integration. So this is a paste a URL simplicity that enables you to easily target things like maps or like big query and it's basically Google turning tool use into a kind of managed infrastructure right it's a step toward reliable govern integrations that are as standard as a protocol like OOTH is today and it directly attacks the most painful part of building agents which is keeping all of those external connectors working secure auditable all the time over time and the strategic play here is subtle but I don't want you to miss it managed MCP endpoints end up being a distribution tools because once connectors are standardized, once they're governed, the competition shifts to who has the richest tool surface plus the tightest billing integration, right? And agent ecosystems are going to look a lot less like prompt libraries in the future and a lot more like the cloud marketplace for capabilities that we see on the cloud side. So, Google is trying to position themselves to own a meaningful chunk of that future market with this move. Next up, OpenAI says prompt injection is not solvable. December 22nd, I don't want you to miss this one. OpenAI published a post on hardening Jet GPT's Atlas browser against prompt injection. The notable part notable part wasn't these security updates. It was the admission. OpenAI explicitly said prompt injection is unlikely to ever be fully solved, especially as agent mode expands the threat surface. So, they shipped updates after internal red teaming found new attack classes, but the framing was really clear. This is an ongoing defensive battle. There's not a way to lock this down forever. And so the industry is kind of conceding a core security truth about agents here. If an agent can read untrusted content and take actions, you are always going to be playing defense. It's just the way it is. And that pushes the entire category toward a seat belt mindset. Constrained execution, approval gates, providence tracking, comprehensive logs, roll back capabilities. And so security is becoming more and more a primitive for user experience in 2026. Winning agent products are going to make safe autonomy feel really normal. So, they're going to start to bring up action plans that you can review if you're using the agent. They're going to start telling you about the explicit scopes the agent has. They're going to start having default deny tool access patterns. And enterprises are going to start to refuse anything that can't explain why it did something. The products that nail this are going to earn so much trust. And the ones that are skipping security right now and saying security isn't a CL first class product, they're going to miss out. Last but not least, Curser announced that they are acquiring Graphite, the code review and collaboration platform. Graphite is going to continue as a product, but the intent is very clear. They want to collapse the boundary between writing code and shipping code. This tells you where Vibe coding is heading. The bottleneck has moved. Generating code got dramatically easier in 2025, but shipping code, review cues, CI pipelines, merge discipline, quality gates, that did not get easier. Cursor buying graphite is effectively a bet that the winning AI dev platforms will own the entire SDLC loop, not just the code editor. In 2026, AI coding assistants are going to become AI delivery systems. So policy checks, test synthesis, risk scoring, code review automation, the editor is just the front door to all that, right? Whoever owns review and merge effectively owns organizational trust of AI code and that's what turns individual vibe coders into enterprise software factories. And that is exactly where cursor is starting to drive. So the personal vibe coding projects of 2025 are going to become easy to manage for a team in 2026. So that's your 2026 starting position. Power constraints are real and they're getting realer. The hardware competition is shifting from chips to systems. Agent security is in frankly a permanent arms race scenario. And the winners are going to be the ones that can make AI infrastructure boring, reliable, and governable because AI is going to be everywhere. With that, let's get to
