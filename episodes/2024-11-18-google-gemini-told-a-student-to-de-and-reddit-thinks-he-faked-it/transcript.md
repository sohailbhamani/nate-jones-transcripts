---
title: "Google Gemini told a student to d*e and Reddit thinks he faked it"
video_id: "4HLxtDm_K_w"
youtube_url: "https://www.youtube.com/watch?v=4HLxtDm_K_w"
substack_url: null
publish_date: "2024-11-18"
duration: "4:35"
duration_seconds: 275
view_count: 732
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "News Roundup"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
entities:
  companies:
    - "Google"
  people:
    []
  products:
    - "Gemini"
    - "Make"
  models:
    - "Gemini"
concepts:
  []
summary:
  []
keywords:
  - "ai-news"
  - "coding"
  - "frameworks"
  - "gemini"
  - "google"
  - "make"
---

# Google Gemini told a student to d*e and Reddit thinks he faked it

what happened at the University of Michigan last week I don't mean student wise I mean in terms of Google and gini and the chatbot that threatened a student with death so long story short the headline that got reported is that Google Gemini was having a chat with a student at the University of Michigan and out of the blue Gemini started to say you should die you're a blight on the landscape why are you here basically being abs absolutely awful to this student but it gets weirder so as soon as that happened and the first news cycle broke which is basically Gemini is doing evil things what is AI doing to us the second news cycle took over and the second news cycle was a little bit more skeptical basically people started to look at the chat and they analyzed the transcript and they said well wait why is the student using the utterance listen in this part of the transcript right before the chat Bo starts to say die die die Etc and they suspect that the CH student was able to sort of jailbreak the llm and get it to threaten him with death and why would you do that you ask because you want attention right and this student has certainly gotten plenty of attention so I actually don't care I don't care whether he was able to jailbreak it or whether it was a spontaneous defect coming from the large language model and Google doesn't care either and the reason I know that is because Google agreed to take accountability for fixing it so Google basically said chatbots should not do this which is the correct position for Google to take and it does not matter how the chatbot did it the fact that the student was able to jailbreak it is frankly just as bad as the fact that it was able to occur spontaneously because at the end of the day either way from a corporate perspective I you're facing tremendous liability and so you need to make it so it's impossible to jailbreak and I really have empathy for Google's Engineers because that's a really tall order this is a chaotic generative system where very very small changes in initial output initial input can result in tremendous changes in output how do you safeguard that system 100% of the time like 99.9% is not acceptable 69 is not acceptable you have to get to 100% and no technical system really is there for anything let alone for generative which is a technology that is notoriously hard to safeguard so we will see what happens I'm sure that Google will figure something out and launch a patch but I don't believe that the problem will be fundamentally solved for generative AI applications because inherently generative applications are chaotic and chaotic applications do weird things they either do weird things spontaneously or they do weird things when you jailbreak them and jailbreaking has become a social engineering act like you go through and you can social engineer jailbreaks and that is perhaps what this student did in order to get Gemini to threaten him if indeed that's what occurred so you will hear both versions circulating Reddit is very keen on the theory that the student did this to himself for attention news outlets are being more conservative and basically saying this happened and it's bad and Google kind of doesn't care either way and it's saying we should fix it it's our problem so the point here is that you should think about generative systems as chaotic and hard to Corral by default and you should plan for policies that assume chaotic representations of data in the long Tales which is a fancy way of saying you should assume weird stuff is going to happen in the long Tales of your chats and you should plan appropriately whether that means rewriting your policies from a liability perspective whether that means imposing extra checks it's probably both and either way generative requires different kinds of safeguards and risk management so there you have it Google's Gemini threaten someone with death and we're all trying to live in the aftermath thankfully I have not yet been threatened with death by my chat bot I try and say please and thank you I hope you do too cheers
