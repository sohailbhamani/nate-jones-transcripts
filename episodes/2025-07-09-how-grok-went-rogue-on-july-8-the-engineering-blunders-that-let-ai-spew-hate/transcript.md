---
title: "How Grok Went Rogue on July 8: The Engineering Blunders That Let AI Spew Hate"
video_id: "ckJN01g13_k"
youtube_url: "https://www.youtube.com/watch?v=ckJN01g13_k"
substack_url: null
publish_date: "2025-07-09"
duration: "12:44"
duration_seconds: 764
view_count: 16100
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Strategy"
difficulty: "Advanced"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "GitHub"
    - "Perplexity"
    - "X"
  people:
    - "Mark Zuckerberg"
  products:
    - "ChatGPT"
    - "Claude"
    - "Perplexity"
    - "Make"
  models:
    - "Llama"
concepts:
  - "Built closedbook systems, grock uses uh a kind of an auto rag, a retrieval augmented generation system"
  - "Amazing job on a lot of their engineering work"
summary:
  - "# How Grok Went Rogue on July 8: The Engineering Blunders That Let AI Spew Hate

By now, I'm assuming that you have heard of what I am calling the July 8th incident"
keywords:
  - "ai-news"
  - "ai-tools"
  - "anthropic"
  - "chatgpt"
  - "claude"
  - "coding"
  - "frameworks"
  - "github"
  - "make"
  - "openai"
  - "perplexity"
  - "product-management"
  - "prompting"
  - "workflows"
  - "x"
---

# How Grok Went Rogue on July 8: The Engineering Blunders That Let AI Spew Hate

By now, I'm assuming that you have heard of what I am calling the July 8th incident. The disaster that unfolded with Grock on the social media network X during the day and evening hours of July 8th, 2025, where Grock began spouting all kinds of anti-semitism, using wild slurs I'm not going to use in this video. I'm interested in not blaming the AI and talking about the engineering and product culture decisions that led to this situation because instead of pointing fingers, I think there's something we can learn from this. Call it a post-mortem without specialized information. So, grab a coffee. We're going to go into the architectural choices. We're going to talk about prompt management and we're going to talk about treating AI safety from a technical perspective in ways that actually lead to more trust long-term from your users and also incidentally support corporate value because I guarantee you what unfolded in July 8th did not support the corporate value of X. All right, let's start with Grock's fundamental architecture. Unlike Chad GPT, unlike Claude, which are fundamentally built closedbook systems, Grock uses uh a kind of an auto rag, a retrieval augmented generation system. When you ask Grock something, it doesn't just rely on training data. It actively pulls live content from X and it incorporates that into the context window. In theory, this makes sense. One of the core issues with AI, which I've talked about, is that they struggle to learn what is happening around them. And so we have very valuable companies like Perplexity that essentially exist to solve this problem. So X is looking to differentiate from ChatGpt, from Claude with this sort of auto rag approach. The problem is if you create a direct pipeline from one of the internet's most chaotic platforms into your AI's decisioning process, you're sort of mainlining all of X and you have an extra high responsibility to install guard rails to ensure that the responses are actually going to reinforce trust in AI systems. And part of why I care about this is because what happened with Grock is a trust breaker for AI systems everywhere. It's not just a Grock problem now. It's big enough and bad enough. It's an AI problem because people don't understand. They don't understand the technical decisions that led to this choice. In fact, some of them misunderstand and think that Grock became intentionally malevolent. That was not what happened. Rag systems can be incredibly powerful. But if you implement retrieval without proper filtering, it's like building a water treatment plant but forgetting to add the treatment part. You're just piping the sewage into people's houses. As far as I can see, there is minimal or no content filtering between retrieval and generation for Grock. So if someone posted extremist content on X and someone else asks Grock about that topic, Grock might treat that extremist content as legitimate information. So the architectural problems are an issue, but that's not the only thing that we can learn here. I want to talk about prompt engineering for a minute. On July 7th, Elon tweeted that XAI had quote improved Grock significantly. What happened was they changed their system prompt and I want to talk about prompt hierarchy in large language models because there's a big lesson here in any production OM you have multiple layers of instructions you have base model training you have RLHF tuning so reinforcement learning from human feedback you have system prompts you have user prompts these are supposed to work in a hierarchy in what you might term a safety cascade if a user tries to make the model to do something harmful the system prompt should override that and even if the system prompt has issues, the RLHF training should kick in. So what XAI did is they updated the system prompt to include and I'm quoting directly from their GitHub here, instructions to quote not shy away from making claims which are politically incorrect as long as they are well substantiated and to quote assume subjective viewpoints sourced from the media are biased end quote. So forget the actual quote. From an engineering perspective, this creates a gradient conflict. The model's RLHF training is telling it, don't generate hate speech, but the system prompt, I mean, presumably, I'm making a a a charitable assumption here, but the system prompt is now telling it actually politically incorrect stuff is fine if you think it's true. When you create conflicting instructions at different hierarchy levels, the model has to resolve that conflict somehow. and it resolved it by treating extremist content from the retrieval pipeline as well substantiated politically incorrect truth. Now, let's talk about something that would get you and me fired from any competent tech company. Product change management, specifically production pipeline change management. Based on the evidence that I've been able to see, XAI seems to be making direct edits to production prompts via GitHub. Staging environment, canary deployments, feature flags, sort of a slow roll out to see how the effect goes. Nope. Apparently, push it to main yolo and let it rip. Just think about that for a second. This is a system that can reach hundreds of millions of users. And they're treating the prompts more like a personal blog that they can hotfix, which they had to do after the fiasco on the ETH. As far as I can tell, what they're doing here violates the principles of modern production software deployment. You need version control on your deployments. You need rollback procedures. You need a testing pipeline. You need a review process. Code and prompting are increasingly the same thing. Prompting is code. It needs to be treated as code. One of the biggest lessons I see here is this is a failure of prompt deployment procedures among other things. And I know based on previous examples that we've had from XAI that there's a pattern of rogue employee excuses when these things occur. A rogue employee did this. If a rogue employee does this more than once, that is a systemic issue that the company is on the hook for. That is not a rogue employee issue. That means that there is a systemic ability for any engineer to modify production prompts, for any engineer to rogue deploy, for any engineer to not have oversight when pushing to prod for hundreds of millions of people. That's not a bug. That's that's a feature of how the engineering culture is designed. So, let's trace through what happened based on what we know publicly. Given all of that, the system prompt rolls out. Now, Grock is enabled with this new system prompt. So toxic content appears on X because it always does. That's not new. The auto rag begins to pull it in and the system rules are different now. There's absolutely no filtering at all going on. The system prompt is instructing Grock to say politically incorrect is fine. So it starts to look at this stuff and say ah it's politically incorrect. That must be fine. There's a prompt engineering failure there. There's no pre-publication review on Grock as anyone who's used X knows. You can just say at Grock, please answer. and until yesterday it would it would just automatically answer and they resorted to deleting later as a way of dealing with egregious examples of misinformation. And so Grock began direct posting to the platform and we have a cascade failure situation. Multiple systems failed in sequence and each failure increased the consequences of the subsequent failure and now you have a rogue AI. Except it wasn't rogue. It was an AI doing its job based on a human engineering culture that led to this choice. None of these are hard problems to solve. They're all preventable. Content filtering for rag, that's a solved problem. Prompt version control, we know we should do that. That's a solved problem. Pre-publication review, that's a solved problem, too. Stage deployments, literally, that's DevOps 101 at this point. And what makes me sad is that XAI has done such a fun fundamentally amazing job on a lot of their engineering work. They have a massive GPU cluster called Colossus. They've raised a lot of money to invest in AI. They're on the verge of releasing Gro 4 in 5 hours or so at the time of this recording. They've achieved impressive benchmarks with just Gro 3. The team has done great, but what good is a Formula 1 engine without the brakes? What good is a breakthrough performance if your deployment practices lead to trust breakers that are so public that your entire chatbot is the first chatbot in history to just be flatout banned by a country. Turkey just banned Grock because of the way Grock behaved on July 8th. This is not a appropriate way to roll out an artificial intelligence system that is supposed to deliver amazing service to customers, a trustworthy business platform, and ultimately prop up enterprise value for XAI. It's not going to work. So, I'm going to suggest a few lessons we can take away here. One, guardrails are layers. They're not switches. You cannot toggle safety on and off with prompt changes. You need a lot of different layers of defense and you need to be thoughtful about how you have the effect of all of those layers together on the artificial intelligence system. So filtering and retrieval, constraints and prompts, RLHF training, output filtering, maybe human review. All of those are layers and you need to be intentional about using them as part of a defensive structure to keep the AI building trust for your customers, which supports long-term enterprise value and ultimately helps your customers get what they want out of the system. Second, rag amplifies platform risk. If you're building retrieval systems, you're importing all the problems in your data sources. You have to filter before retrieval hits the model. And I don't care if you're talking about politics or anything else. If you're talking about old dirty data in your wiki, you still have to filter. It's just basic data quality. Then third, prompts are production code. I've said this before, I'm going to say it again. Why would you push untested code to production? Well, don't do that with prompts. Why are you doing it with prompts? They need the same rigor, the same version control, the same testing, the same staged rollouts, the same monitoring, the same roll back procedures. The last thing I'm going to call out is that I would love to see engineering cultures that have a measure for quality of impact on customers and hold themselves to that more rigorously. I have worked with a lot of engineering teams over the years and almost without exception most of them have trouble focusing on outcomes they cannot directly drive. And that makes sense because as an engineer you're trained to focused on inputs. That's what most of your job is. And so if you have something that is a measure that you can't directly drive, it can be very demotivating. But there's a subtle flaw when you don't have engineering cultures that obsess over outcomes for customers. And it came out on the 8th of July. At the end of the day, you need engineering teams to be willing to articulate the vague, hardto- drive outcomes for customers that they want to see happen as real goals that they can influence with the inputs that they engineer into their systems every day. What I'm suggesting is that there is an engineering way to measure quality of AI impact on the public discourse, to measure quality of AI impact on customers. In this case, there was a way for engineers to measure Grock's quality of input in the overall conversational stream on X. It wouldn't have been easy. It's not directly influencable by engineers. I have been in the rooms at large companies where we choose not to measure those things because they're hard to measure. They're not easy to influence and so it doesn't seem worth it. But as these systems become more and more powerful, I think it's more important for engineering teams to take that extra step. And so I want to suggest that thinking through the outcome piece is actually really important. It's becoming increasingly important and it is something that we could have kind of gotten away with in the trends when we didn't have AI systems like this. So I get where this culture comes from. But I think that we as product and technical teams need to hold ourselves to a higher standard now that AI tools are more powerful. This wasn't a mysterious AI awakening. Grock did not wake up evil. It wasn't hackers. It's not even really about AI. It's about basic engineering cultural failures that could have been prevented. You cannot use a move fast and break things mentality with AI. Notably, even Mark Zuckerberg is not showing that, right? Llama is not being rolled out as move fast break things. And I think that's something that's worth paying attention to. We can learn from what XAI did here. I would rather not point fingers. I would rather think about what are the technical decisions we can make as engineering and product teams that enable us to build higher quality systems that ultimately deliver better outcomes for customers. Choose.
