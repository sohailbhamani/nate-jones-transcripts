---
title: "Anthropic's New Benchmark Changes Everything—Most People Will Miss Why"
video_id: "X_EJi6yCuTM"
youtube_url: "https://www.youtube.com/watch?v=X_EJi6yCuTM"
publish_date: "2025-12-29"
duration: "10:23"
duration_seconds: 623
view_count: 41895
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  - "AI agents"
  - "super exponential AI"
  - "METR benchmark"
  - "future of work"
  - "AI strategy"
  - "Claude Opus 4.5"
  - "automation at work"
  - "AI jobs"
  - "large language models"
  - "agent delegation"
  - "AI career advice"
  - "upskilling with AI"
  - "Gemini"
  - "ChatGPT"
  - "power law distribution"
  - "AI workforce transformation"
  - "domain expertise AI"
  - "agentic work"



# AI-enriched metadata
content_type: "News Roundup"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
  - "Founders"
entities:
  companies:
    - "Anthropic"
  people:
    []
  products:
    - "Claude"
    - "Gemini"
    - "Make"
    - "Opus"
  models:
    - "Opus 4.5"
    - "Gemini"
concepts:
  []
summary:
  - "# Anthropic's New Benchmark Changes Everything—Most People Will Miss Why

We are on the super exponential timeline for AI agents and I want to explain what that means and why it's super important that"
keywords:
  - "ai-agents"
  - "anthropic"
  - "career"
  - "claude"
  - "coding"
  - "frameworks"
  - "gemini"
  - "make"
  - "opus"
  - "product-management"
  - "tutorials"
  - "workflows"
---

# Anthropic's New Benchmark Changes Everything—Most People Will Miss Why

We are on the super exponential timeline for AI agents and I want to explain what that means and why it's super important that we all pay attention to it. MER is the model evaluation and threat research company. It's a nonprofit. It's dedicated to understanding how models perform and they are famous for producing a graph that shows how long models can do useful agentic work for at a time. It's a little bit of a confusing graph to understand, so I'm going to explain it really simply. Basically, they take a task and they measure how long a human takes to do that work task. And then they want to find out if the AI can do that task with at least a 50% likelihood of success. Why 50%, because they had to pick a number somewhere. They also measure it at 80%. And we'll get to that. PTR is important because it does not top out. And so if you have a lot of these these benchmarks like Swebench is an engineering one, it tops out at 100% and we're already in the like way way up at the top it doesn't matter like you can go from 91 to 93 and you don't really get a sense of how the models change. TR is different because that graph has no top end. It can just keep doing more and more work and that allows it to show super exponential progress. And one of the biggest debates of 2025 was are we on an exponential time scale with AI or are we on a super exponential where it's increasing faster than exponentially. It seems like we're on the super exponential trend line. And one of the things that made us think that is this latest result from Opus 4.5 which shows over 4 hours 4 hours and 45 minutes almost 5 hours of human equivalent work done at a 50% likelihood of success. Now the 80% mark is also measured and it is 2728 minutes for Opus 4.5 which you might think oh that's not that far but keep in mind it was not that long ago that we were 1 minute 2 minute 10 minute 30 minutes and now we're up to almost 5 hours and that is the point of a super exponential curve. We are on a doubling rate every 4 to 4 and 1/2 months right now. And so if the number is 50% complete, but the time horizon is four almost 5 hours, we're going to be at 10 hours by the end of Q1, we'll be at 20 hours by the end of Q2 into Q3, and we may be at 40 hours by the end of the year or past. And that is why we have to pay attention to this. Super exponential gains suggest that we have hit a selfreinforcing flywheel with AI. And that is indeed what we hear out of model makers and that is why 2025 was the last normal year. We are going to see really really weird progress from AI in 2026 and every year after because AI itself is starting to reinforce AI systems. We're bringing AI in to help train AI systems. Now that is going to become more and more automated. We are going to have capabilities that AI itself helps to grow speeding up the whole process and all of that is going to allow us to continue to make progress on these tough tasks that don't have an upper limit. And this matters because really our ability to do meaningful work is going to be determined by whether or not we can define useful high taste highquality work that an AI can do over a period of time. Do you have something for an AI that would take you a week to do? Maybe it's your taxes. I don't know. But that's going to increasingly become the question. And if you don't, then the question is going to be what does it take for you to get there? What does it take for you to gain the skill to assign that work? Because in a super exponential world, the skill we need to learn is also super exponential. The people who figure out how to assign agents work now in January and February and March are going to have a much easier time learning how to continue assigning agents work when the agents can do much harder stuff. Whereas if you wait and say, "I'm going to catch up. I've scheduled this for Q2 or Q3 next year. That's my AI quarter." Good luck with that. Like it doesn't work that way. There will be people who are running circles around you because they can assign their agents a week's worth of work. And once you can assign your agents a week's worth of work and spend up two or three of them, look at how much more productive that makes you. You're going to be running circles around people. And that that is the power law distribution world we're going to live in. Super exponentials create power laws. So power law is that the idea that the world we live in is not normally distributed. A normally distributed world, most people are on the average, a few people are on the tails. Einstein's way over here, right? But in a power law world, just a few people are going to be able to do a tremendous, tremendous amount. And it's not because they're necessarily going to have lots of money to do it. It's because they have the skills to do it. AI is going to disproportionately reward skill development where it's related to artificial intelligence and everything else. People are going to start to lose traction. If you are looking to make a dent in your career, I would look less in 2026 at your job family's traditional requirements and look more at where can an agent do a meaningful amount of work for a week in this traditional job family area and how can I make sure I set myself up so I know how to define and assign that work, know how to hold it accountable, know how to put good taste down so I know what excellent looks like in that work, know how to intervene, keep the agent on track, have the technical foundations necessary to define and set up an agentic system. This is going to become more and more relevant for all of us. The technical skill sets are going to spread across the job families. The non-technical skill sets are also going to spread across the job families. Engineers who traditionally just had to do code are going to have to have some business fluency and customer fluency now because they have to be the ones with good taste when they're architecting systems. And frankly, they now have to architect systems that non-technical people can contribute code to. So just that one shift, that ability of agents to do work over time is going to multiply the impacts across all of the rest of us. Having agents that work longer means all of our jobs are going to change forever. And you might think I'm like a hype person. This is not being me being hypy. This is me just talking about the reality that we are on a super exponential curve. Humans are bad at estimating super exponential curves. And so I just want to make it really concrete for you. I do think there is no way that work will not change for everybody if we are in a place where it's 5 hours and doubling every four months. Because you look at it by April you're going to be at 10 hours. By what July September you're going to be at 20 hours by December you're going to be at 40 hours maybe. Right? Maybe it's not even like it's just it's going to be crazy. Are you able to delegate a week's worth of work? That is that is the question of 2026. We will all have to let go of a lot. We will have to let go of our traditional understandings about career progression. We'll have to let go of our traditional understandings about job families. What job families know and what they don't. We are going to have to be outcome obsessed, ownership obsessed. The work of the future is going to reward people who are ownership and outcome obsessed because that's where human value shows up. It's when we make sure that what's made is actually relevant for people, is actually useful, is actually good. It's not just vibecoded slop. There will be lots of vibecoded slop. In fact, I would expect it to 100x in 2026 because you can ask your agent to do a lot of terrible, terrible things. It's going to be up to you to decide that the agent's work is worth it. That you are assigning the agent and the agent is doing good work to get meaningful work done that compounds over time. The strategy rewards used to acrue to leaders. Strategy is now an individual thing because you are effectively a strategic manager of a team of agents or you will be in 2026. You can make them yourself. There will probably be startups that market them to you. But either way, you will end up with a team of agents working for you. Do you know how to manage them? Do you know how to lead them? Do you know how to drive them to develop compounding advantage over time? That used to be a question for directors and above. It's not for if now it's for everybody. Everyone will need to be able to do this and the people who can are going to look like they can do anything like that. The span is going to be incredible because they're able to leverage their own domain expertise and expand their scope of impact from there. I do not mean that you can do anything that requires deep domain expertise that you do not have. There are still going to be real value that you can't get to. there's going to be real value you can't get to just by adding agents. So, for example, if you are a lawyer and you have decades of experience, agents are going to transform the legal profession and how you work, but it it's not going to transform it to the point where I, as a non-awyer, can come in and do work for a white shoe law firm and get exactly the same quality of work done at the end as the lawyer who's got decades of experience. there's going to be a reward for understanding the business deeply that will show up in your ability to direct AI agents toward useful ends. And so as much as it may seem like I'm saying the agent can do work, we won't do any. What I'm really saying is our domain expertise is worth more and more, but boy do we have to be smart and leverage it really, really differently to get where we need to go in 2026. And that's going to change all of our skill sets. We're all going to have to learn together. We've never gone through this workflow and workforce transformation before. So, we're all going to have to just jump in and figure out how to do it together. But, I do think it's real. I do think it's coming. And I do think the key is that super exponential graph. Opus 4.5 was just the latest getting to 5 hours. It won't be the last. It's not like Claude has a special, you know, Claude doesn't have a special monopoly on this, right? We're going to see this from Gemini. We're going to see this from Chad GPT. We'll see it from other model makers as well. We will continue to see exponential gains from agent working time in 2026 and that will change the way all of us have to do our work.
