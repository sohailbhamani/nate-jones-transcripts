---
title: "Amazon Fired Their AI Chief. Here's Why It Took So Long (Plus 5 Newsworthy Moments in AI This Week)"
video_id: "EaMz3g1OYPA"
youtube_url: "https://www.youtube.com/watch?v=EaMz3g1OYPA"
publish_date: "2025-12-20"
duration: "10:32"
duration_seconds: 632
view_count: 30156
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  - "AI news"
  - "artificial intelligence"
  - "semiconductors"
  - "chip war"
  - "ASML"
  - "EUV lithography"
  - "Zeiss lenses"
  - "China tech"
  - "export controls"
  - "AI chips"
  - "enterprise AI"
  - "AI implementation"
  - "AI agents"
  - "Exa"
  - "people search"
  - "search benchmarks"
  - "LinkedIn"
  - "ZoomInfo"
  - "data scraping"
  - "Meta AI"
  - "SAM Audio"
  - "audio separation"
  - "hearing aids"
  - "Amazon AI"
  - "AWS"
  - "Andy Jassy"
  - "Trainium"
  - "AGI"
  - "robotics"
  - "vision-language-action"
  - "Physical Intelligence"
  - "Pi0"
  - "egocentric video"
  - "robot learning"
  - "Tesla Optimus"
  - "Google RTX"


# AI-enriched metadata
content_type: "News Roundup"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
  - "Founders"
entities:
  companies:
    - "OpenAI"
    - "Anthropic"
    - "Google"
    - "Meta"
    - "Microsoft"
    - "Amazon"
    - "Apple"
    - "Tesla"
    - "Salesforce"
    - "Adobe"
  people:
    []
  products:
    - "Runway"
    - "Make"
  models:
    - "SAM"
concepts:
  - "Reshape your business"
  - "Why they're making that play"
summary:
  - "Here's Why It Took So Long (Plus 5 Newsworthy Moments in AI This Week)

I read more than 20 hours of AI news this week to bring you the stories that matter in just 10 minutes"
keywords:
  - "adobe"
  - "ai-agents"
  - "ai-news"
  - "ai-strategy"
  - "ai-tools"
  - "amazon"
  - "anthropic"
  - "apple"
  - "career"
  - "coding"
  - "deep-dives"
  - "frameworks"
  - "google"
  - "leadership"
  - "make"
  - "meta"
  - "microsoft"
  - "openai"
  - "product-management"
  - "runway"
  - "salesforce"
  - "tesla"
---

# Amazon Fired Their AI Chief. Here's Why It Took So Long (Plus 5 Newsworthy Moments in AI This Week)

I read more than 20 hours of AI news this week to bring you the stories that matter in just 10 minutes. Let's jump in. Number one, China's Manhattan project to reverse engineer ASML and break the western ship chokeold reached a key milestone this week. On the 17th of December, Reuters published an investigation exposing that China had a six-year government coordinated effort to build a domestic extreme ultraviolet machine. Say that five times fast. The quarter billion dollar school bus-siz tools are required for manufacturing AI chips and are currently monopolized by the Dutch firm ASML. The effort was launched back in 2019 and they have just now reached the point where the prototype can generate EUV light. This does not mean that it's able to make a chip. They need lenses to make chips among many other breakthroughs. The lenses are a key choke hold because the only lenses that work in these machines, I am not making this up, are Zeiss lenses. Zeiss lenses are so precise that if you stretched a Zeiss lens over the North American continent for thousands of miles, it would vary by.1 millimeter. That is how precise they are. This all matters because AI has been framed as a great power competition. So, Western companies have been looking to impose blockades on Chinese chip supply. China has been looking to build its own machine to break itself free from dependence on western chip supply stacks and the silicon stack that has been engineered and built in the west. This also extends into mining into competition over tooling. You'll see this play out in rare earths conversations. What you should be watching for is an intensification of the great power conversation because that can be dangerous geopolitically and it can also indicate potential progress on the Chinese front with this EUV effort. I would be watching for any indication of industrial espionage at Zeiss, any stories leaking there or any indication that some kind of chip prototype or pilot production run has been achieved domestically in China. I would expect at current rates that would happen in 2027 or 2028. Next story. The market is finally admitting that AI needs implementation. On December the 16th, Reuters published a separate analysis based on interviews with lots of CEOs. And what they discovered is what I've been preaching for a long time. AI is not something you can plug in and just hit go on, even if it will fundamentally reshape your business. It turns out that was a surprise to many of these executives. They report that they built systems for writing and coding and Q&A and they have really struggled to get more complex domain specific tasks because they're struggling with their data business pipelines. They're struggling with business logic encoding with tool integrations. Yeah, it turns out it's not magic guys. You actually have to work. All kidding aside, do watch for a pivot in vendor marketing from magic co-pilots that just work to more detailed reference architectures. I think that we are reaching a point where many seuite buyers are having buyers remorse and are tired of the cheap promises that a lot of vendors have made. Vendors are going to need to respond frankly with more detailed commitments around how they will integrate with existing stacks. You cannot sell magic buttons anymore in 2026. Next up, EXA launches people search. Search has been something that we have not had good evaluations for or a good sense of what good looks like even. Exa is trying to change that with AI powered people search. They claim they have the most accurate AI powered people search in the world now and more than a billion of us are available on exa.ai as searchable entities. Exa has also started to pioneer the way by publishing benchmarks that evaluate precision, recall and ranking quality which we've sorely needed in search. Now the team is claiming that this is how a lead generation or marketing effort could search for accounts, could search for experts, could search for candidates. They are seeing this as a B2B play. Obviously, people may be worried about privacy as well. Because the people search feature is not only available to technical users and business users, it's available to all of us. Just as you can search for a VP at Salesforce, you could also search for your ex. The announcement comes as the broader AI agent narrative is shifting from hype to implementation reality with people search addressing a really crucial need. Most serious B2B agents whether for sales development or recruiting or partnership sourcing are going to eventually need to find and reason about people and ex's product needs to be able to address that. So that is fundamentally why they're making that play. Exus sees itself as a foundational building block in the agentic web and they need agents that can reason about people as a result. Watch for Zoom Info, LinkedIn, and others complaining about the data scraping practices of Exa. In some early tests, this looks heavily like a LinkedIn scraping tool, and LinkedIn has historically not been very tolerant of that. So, I would expect Microsoft's lawyers to come have a chat. I would also track whether X exa exposes any higher level abstractions beyond the raw API such as find similar prospects to this ICP or rank these candidates by domain expertise. You know, common queries that would move their product up the stack from just raw search to effectively workflow primitives somewhat similar to how Stripe eventually evolved from just being a payments API to full financial infrastructure. I would also monitor and see if this eventually pressures Zoom Info or LinkedIn to respond with their own agent-friendly APIs, which neither of them has been willing to do to date. Next up, on December, Meta's AI team introduced SAM audio, a unified multimodal model for audio separation that isolates any sound from a complex mixture using text prompts such as isolate the guitar or point to an object in a video or a time span selection. SAM audio basically enables you to perceive, isolate, and pull out any sound in the ambient environment. This has lots of implications for folks wearing hearing aids, but it also has lots of implications for anyone who's trying to sample or edit music. Why is Meta working on this? Because Zuck's long-term vision is that if you have a wallet, you can advertise on his platforms. And that means that he has to be able to take care of everything in the ad creation process for you. That includes the visuals, that includes the audio, that includes the video for video ads. And so it makes sense that his teams are innovating in this area. I would watch to see if Sam audio gets actually adopted in existing creative tools such as the Adobe stack or Final Cut Pro. I would also look to see if it gets adopted in, as I was calling out, accessibility software, real-time speech isolation for hearing aids, transcriptions, etc. and check to see whether competitors like OpenAI or Runway end up shipping comparable audio editing models or whether they end up partnering with Meta's open ecosystem. Next, also on the same day, Amazon announced a major reorganization of its AI efforts. In a memo from Andy Jasse, you might ask what AI efforts and you would be correct. This is why we are reorganizing. Peter Dantis, a 27-year AWS veteran who's led infrastructure, compute, applications, and networking, will now head a unified Amazon AI or that includes the artificial general intelligence team. I bet you didn't know they had one. A custom silicon development team, the tranium chips and so on, and quantum computing reporting directly into Jassie. Roit Prasad who built Alexa and led Amazon's AI initiatives previously is now leaving at the end of the year and Amazon has I think showed him the door because they keep lagging rivals like open AI and generative AI momentum and lagging is frankly kind. There is no AI model that anyone is talking about at any level consumer or business that is Amazon's and considering the fact that Amazon put 15,000ish engineers on Alexa at its peak and they missed the ball this badly. This is overdue, right? This is late. This is Apple level missing the play. I would watch Peter Dantis' first major announcements here. Whether he's got custom silicon road maps with tranium, whether he's got an AGI team product launch to put together, he'll be under pressure to come to reinvent with something to say that is significant. I would also keep an eye on whether Amazon's potential OpenAI investment materializes because that could mark a strategic shift from Amazon builds everything which is sort of how they historically work to we're going to partner with Frontier models including anthropic and open AI and just choose to own the infrastructure much more similar to Microsoft's play. Last, but certainly not least, on December the 17th, physical intelligence, the startup, posted about discovering an emergent property in their vision, language, action models named PI0, PI 0.5, and PI 0.6. I'm not making those names up. As they are scaling pre-trailing, pre-training for robots up, the models turned out to be able to learn automatically from egocentric human videos. But what I mean by that is videos captured from wearable cameras that are the point of view of the human. So let me say that again for you because I think you just need to get it. The machine learning models, the the visual language action models were able to emergently learn from human videos. They developed the capability as pre-training scaled. When we talk about pre-training not hitting a wall, this is what we mean. Nobody taught them specifically to watch human videos and imitate them and learn how to map them onto robotic action. They just learned. It turned out that fine-tuning the PI 0.5 model with human videos doubled performance on depicted tasks compared to robot only data. An experiment showed that this transfer improves with robot data scale and diversity which is visible in aligned latent representations where human videos look like robot demos in highdimensional space. This is one of the biggest stories of the year because if we can unlock robotic learning from human POV, we are going to unlock hundreds and hundreds and thousands of applications of humans doing work for robotic models to learn from. So I would watch to see whether humanto robot transfer results replicate across other VLA architectures. Google has an RTX architecture. Tesla has the Optimus stack. And I would watch to see how quickly physical intelligence is able to scale past the Pi 0.6 range with large human video data sets and whether this enables step change improvements in robot generalization and sample efficiency by mid next year. My expectation is that figuring this out is going to be a big unlock for industrial robotics in 2026. And that's all the news we got, folks.
