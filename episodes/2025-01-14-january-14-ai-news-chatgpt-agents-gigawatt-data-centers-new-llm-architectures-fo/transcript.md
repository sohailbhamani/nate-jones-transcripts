---
title: "January 14 AI News: ChatGPT Agents, Gigawatt Data Centers, New LLM Architectures for Longterm Memory"
video_id: "yb4JIvM9-Ls"
youtube_url: "https://www.youtube.com/watch?v=yb4JIvM9-Ls"
substack_url: null
publish_date: "2025-01-14"
duration: "3:52"
duration_seconds: 232
view_count: 1980
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "News Roundup"
primary_topic: "AI Agents"
difficulty: "Advanced"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "OpenAI"
    - "Google"
    - "Nvidia"
  people:
    []
  products:
    - "ChatGPT"
    - "Projects"
  models:
    []
concepts:
  []
summary:
  []
keywords:
  - "ai-agents"
  - "ai-news"
  - "ai-tools"
  - "chatgpt"
  - "coding"
  - "frameworks"
  - "google"
  - "leadership"
  - "nvidia"
  - "openai"
  - "product-management"
  - "projects"
  - "workflows"
---

# January 14 AI News: ChatGPT Agents, Gigawatt Data Centers, New LLM Architectures for Longterm Memory

four pieces of news for today January 14th number one the Biden Administration executive order the US government is pushing to build gigawatt scale AI data centers on Federal Land so these projects would supposedly use clean energy and americanmade semiconductors that is the Hope expressed in the executive order I think the fundamental challenge there is that the architectures used for cutting Edge graphical processing units that are in AI data centers have never been built in the United States to date even the new production push in Arizona for the 4 nanometer architecture is not considered Cutting Edge 3 nanometer is considered Cutting Edge and two nanometers coming next and so this is one of the tensions in the executive order I see the idea of enabling gigawatt scale data centers on Federal Land I am not sure how it actually plays out in practice especially with a new Administration coming in so we we are going to have to see number two Nvidia SAA Sana it's a new AI tool that generates 4K images locally on your machine you don't need uh a cloud install of anything to generate the images they're 4K they're very nice quality and the key thing is they're extremely fast I'm still playing with it but it is shocking how fast it's able to generate professional grade visuals like it's much faster than mid Journey number three chat GPT uh and open AI have launched tasks and scheduled tasks specifically for the 40 model of chat GPT it lets you automate particular workflows you do regularly the one that I like to think of is I send a marketing email every Wednesday while now I can have chat GPT one remind me to send it by starting a chat and two encode in the chat the usual inputs I need to send it so I can accelerate my way through it's a baby step in the direction of Agents from openai finally a research paper from Google Titans the question there is can Google actually implement this at scale I released a separate video on this earlier this morning you can go check it out I don't want to repeat what I said there I think the question I have as I continue to read this is whether or not Google is already employing this to try and break the limits of the context window Google has been on the bigger side for context windows and on the weaker side for quality of llm response for a while now that's anecdotal but I've heard it from a lot of people and I think it's really interesting that the paper that they released is about larger context Windows longer memory and implies moving away from Transformer architecture Ure which tightens up the relationship between tokens and would theoretically lead to more contextual relevant responses if they were already implementing a version of Titans and just hadn't talked about it till they released the paper it wouldn't surprise me a ton now I will say I don't know that I'm not at Google it is possible this really is novel and hasn't been implemented into any production system yet uh they are certainly claiming excellent retrieval from Titans but that is different from excellent contextual responses in reasoning across an extremely large body so if it was a 20 million token window would you actually be able to reason across all of that using Titans I don't know and so that's the question I have as I look at the Titans paper I'm still digesting it curious for your thoughts uh but that's the news for today
