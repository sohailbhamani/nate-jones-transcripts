---
title: "Master Perplexity Prompting -- Why It's Different from ChatGPT + Demo"
video_id: "05RRGiF7QC0"
youtube_url: "https://www.youtube.com/watch?v=05RRGiF7QC0"
substack_url: "https://natesnewsletter.substack.com/p/unlocking-perplexitys-power-proven?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true"
publish_date: "2025-10-28"
duration: "20:22"
duration_seconds: 1222
view_count: 13053
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  - "AI strategy"
  - "large language models"
  - "perplexity"
  - "chatgpt"
  - "openai"
  - "promptengineering"
  - "prompting"
  - "contextengineering"
  - "futureofwork"
  - "workingwithai"
  - "LLMs"
  - "AI agents"
  - "prompt engineering"
  - "ChatGPT"
  - "Anthropic"
  - "Claude"
  - "Perplexity"
  - "retrieval augmented generation"
  - "research mode"
  - "agentic RAG"
  - "Perplexity Spaces"
  - "Perplexity Labs"
  - "academic mode"
  - "AI search for teams"
  - "citation checks"
  - "Perplexity prompt strategy"
  - "avoid AI hallucinations"



# AI-enriched metadata
content_type: "Tutorial"
primary_topic: "AI Tools"
difficulty: "Advanced"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "Anthropic"
    - "Google"
    - "Microsoft"
    - "Perplexity"
    - "LinkedIn"
    - "X"
  people:
    []
  products:
    - "ChatGPT"
    - "Claude"
    - "Claude Code"
    - "Gemini"
    - "Sora"
    - "Perplexity"
    - "Make"
    - "MCP"
    - "Sonnet"
  models:
    - "Sonnet 4"
    - "Gemini"
concepts:
  - "A parametric answer engine, which is a fancy way of saying chat gpt's default is to go and look inside its own training data and its weights in the model for an answer for your question"
summary:
  - "# Master Perplexity Prompting -- Why It's Different from ChatGPT + Demo

How do you search with AI and make it good"
keywords:
  - "ai-strategy"
  - "ai-tools"
  - "anthropic"
  - "chatgpt"
  - "claude"
  - "claude-code"
  - "frameworks"
  - "gemini"
  - "google"
  - "linkedin"
  - "make"
  - "mcp"
  - "microsoft"
  - "openai"
  - "perplexity"
  - "prompting"
  - "sonnet"
  - "sora"
  - "tutorials"
  - "x"
---

# Master Perplexity Prompting -- Why It's Different from ChatGPT + Demo

How do you search with AI and make it good? That's what we're going to look at today. We're going to look at prompting for searching on the internet. We're going to look at the best tool for that, which is perplexity. I'm going to give you a guide. It's very different from traditional prompting. So, let's hop in. First, how does Perplexity work? This is often misunderstood, so I want to actually explain it clearly. Perplexity is a search engine like Google, but it's AI native. It specifically uses retrieval augmented generation as its fundamental architecture. That means it retrieves relevant documents, extracts paragraphs, and uses this information to craft answers with citations. So the pipeline looks like external documents across the internet are embedded. They're stored. Every query triggers a fresh retrieval of relevant documents. But there's an important nuance here. If you are using Perplexity's research mode, which we will see in a moment, I'll show you it. Then you have a new approach using the same architecture. And I want to explain it sort of in layman's terms. It's called a gentic rag. And what it means is research mode will perform dozens of searches, read hundreds of sources, and do multiple passes across the rag architecture to ensure it finds the best possible answer. It basically takes the effort level on perplexity and turns it up to 11. That's how perplexity works. It's very different from Google, right? Because Google just finds you an answer. But what is less understood is that it's also very different from chat GPT. Chat GPT is fundamentally a parametric answer engine, which is a fancy way of saying chat GPT's default is to go and look inside its own training data and its weights in the model for an answer for your question. It does not go out and look at the internet by default. And by the way, that is why chat GPT doesn't know about new chat GPT instances. Right? If you ask Chad GPT, it will often give you the wrong answer when you ask it what the current Chad GPT model is. It's not just Chad GPT that does this. Claude does the same thing. Gemini has done this. The reason why it's not some diabolical plot. It is that they are parametric answer engines and they look inside their weights and perplexity looks outside. It looks at the internet as a whole by default. It's like imagine a world where you have an answer engine in chat GPT that looks inside the house first inside your own weights or you have a choice like perplexity that looks at the whole world first and isn't necessarily focused on reasoning first. That's the difference. And so that shapes how and where we use it. And it also profoundly shapes our prompt strategy. Let's get into the prompt strategy piece. First, you need to think of prompting with perplexity as as a little bit goes a long way. Just adding two to three words of critical context can dramatically improve the value of relevant results. I'm going to show you an example here in a moment. Basically, if you have a search like climate models, you're going to get all the semantic results from the entire internet associated with climate models in whatever order Proplexity is able to find it. If you say climate prediction models for urban planning, you're going to get a very precise pull. The thing that I want you to remember is that that doesn't mean you have to use a long prompt. In fact, on average, perplexity prompts are much shorter than chat GPT prompts. And I'll show you that as well. Principle number two, this is this is another non-obvious prompting strategy. You want to avoid what is called fhot prompting. So fshot prompting gives the model examples and I encourage it often when you are using chat GPT but don't do this when you're using perplexity and the reason why is that perplexity will overindex on those examples and dredge up only things related to those examples from your fshot prompt. So if you say me examples of French architecture like the Louvre, you're only going to get museums like the Louvre. You're not going to get anything else about French architecture because of how fot prompting works with Perplexity's architecture. Another non-obvious prompt strategy, you want to use the exact parameters for search behavior control that are embedded in the API. And I realize that that can be a lot if you're not a technical person. So, I'm just going to tell you there are a few that are pretty obvious that you can use without being a technical person. Like limit your sources and say what they are. Filter by date is something you can do in plain language. Adjusting search depth is something you can do directly in the prompt as well. The idea is don't be vague about matters that are in the API. So if you say only search recent sources, that's going to be much less helpful than using a date filter. And you can use the date filter in text. It's even stronger to do it in the API if you happen to be a developer. But regardless, in practical terms, you see a huge jump in quality when you're more specific about things that perplexity is wired to care about, like exact dates. A fourth non-obvious choice is to demand multiple perspectives on the thing you're looking for very explicitly. So instead of saying, "What are the health benefits of X?" say, "Compare findings from at least three peer-reviewed studies on X and ensure that you note conflicts in conclusions that are relevant for understanding X's effects." You see how I'm much more specific there? How I demand a degree of disagreement in the findings. This focuses the model on finding triangulation rather than just converging on a single source synthesis and just paring that. It ensures that you get a wide enough search parameter or a wide enough search scope that it's actually useful. Another non-obvious strategy, progressively deepen. This is not something that you really get to do in chat GPT or in Google the same way. Treat perplexity like a conversation where you are starting with a root question to explore and every answer opens up new questions that you can thread. So you want to intentionally if you're exploring a space start broader than you would necessarily with chat GPT and then you want to iteratively drill down with increasingly specific and actionable follow-up. So the first query kind of maps the territory and then you want to get into something that is like a promising path that is useful for you. This is a very different approach than I find ch sort of prompting chat GPT or claude where you want to bring the intent into a very structured initial prompt and really drive the entire conversation. It's not that way in perplexity. You have room to evolve because you're essentially threading the search engine through the rag architecture to find a particular area that's interesting to you as you discover the conversation together. Another non-obvious technique, specify output constraints. If you specify output constraints, you are more likely to reduce hallucinations. So, as an example, please provide evidence. For every claim you make here, please list specific section references or page numbers so I can check your work. This forces perplexity to verify claims at a granular level rather than assuming it can make broad attributions if it finds two or three different sources and just gloms onto them. Last but not least, actually we have two more two more non-obvious prompt techniques. Use focus mode really strategically. So for example, if you are an academic for peer-reviewed sources or looking for social sources, those are things that you can turn on as particular modes in perplexity. I'll show you in a moment. You want to use that in the middle of the conversation to force a reset of the model's thinking when you are trying to get it out of a rut. So if you're in the middle of a conversation, you're talking about French architecture and you feel like the model isn't taking a historian's perspective, you could go to academic mode in the middle of that conversation without resetting and it would force the model to jump and reset a bit. And that is actually very different from chat GPT because typically you would want to wipe the context window. But in this case you are just shifting the approach in the rag structure that perplexity is navigating and that's different from wiping the context window and starting over with a parametric answer engine like chat GPT. They work differently underneath. So your techniques are different. Okay. The really the last one for a non-obvious technique create spaces with custom instructions where you have repeated workflows that touch the internet. So for example, if you upload reference files on competitor intelligence, you can have a space with a standing instruction that says structure all responses as current state competitive positioning, emerging threats and strategic implications because that space is your competitive intelligence headquarters. That's an example of the kind of internet first project space that perplexity excels at. Another example of something like that. This gets into using labs, which is an a way of using perplexity to construct reports. You want to focus perplexity on internet first use cases where doing a lot of research is going to enable perplexity to come up with the kinds of information that you only get if you are leaning in to publicly available documents on the internet. And so competitive intelligence is a good example. Stocks are a good example. Equity and financial analysis, news is a great example. And the whole product of labs and and spaces, which are two separate ways to organize information. Labs is more focused on creating a nicel looking report. Spaces is more focused on giving you a standing spot for your instructions and a continual workflow. But they're both internet native and that's what you have to keep in mind and that's what diff differentiates them from Chad GPT. Let's have a look at an actual perplexity search result. Okay, this is the first example I want to show you. This was a very simple, I would call it an unhelpful search in Perplexity. Find me recent news on AI. I give it no constraints. I just tell it to go find things. It's very vague. It gives me a lot, right? It talks about major product launches. It mentions things from Sora uh to apparently an update to Chrome, which is kind of random. We can already see the quality decaying here. It mentions nine billion dollars to build energy efficient AI data centers in Oklahoma, which is perhaps not the top infrastructure news I would have picked out given anthropics deal this week with Google. Um, it gets into healthcare and science advances, which are not necessarily super related to what I was asking for, but I didn't communicate my intent. Um, and then it gets into really vague stuff that isn't date specific, like AI investing and spending. Overall, this is exactly what we would expect given the level of specificity we gave the model. Like we we were not helpful and so we kind of get what we pay for there. Now let's look at a much more specific query. Please find me a diverse set of well-grounded novel updates on AI within the last couple of weeks, i.e. since a specific date that are specifically focused on the build use case. In other words, what has happened in AI for builders in the last two weeks or so? Surprise me. Right off the bat, we get more useful answers. We get a note on agent kit, which is absolutely apppropo, but it notes that it was before October 10th. It is paying attention and trying to be helpful, but it's noting that this might be on the edge. And I love that specificity. GPT5 Pro becoming available is a great one. Sor 2 API access is relevant. Enthropic's agentic coding push, so it catches clouded code on the web. It catches claude for life sciences and claude memory. Those are both relevant. It has a slightly weird one, anthropic opening a soul office. Not sure why it matters. Um, but then it makes a case, right? It says it's the number of weekly cloud code users in Korea is up. I didn't know that. That's super cool. Um, talks about Google, talks about Microsoft. This is a much more detailed response. And then it gets into stuff that I never would have found with the other search. It talks about SNIK and Windsurf in Devon um, and sort of how they're partnering together on security scanning in the Windsurf IDE. It talks about open source convergence and how uh we're starting to see near parody with Cloud Sonnet 4.5 and open source models. Uh we're talking about Perplexity's browser and how it went free, but it notes it was outside the window. Full MCP support for Chat GPT developer mode, which I knew about, but has really gotten slept on. It's a big deal. Um and then overall it gives me an assessment. I love this. There's so much to dive in because now I can say I'm really curious to learn more about AI build culture in Korea, especially around claude code. Can you please summarize a diverse set of perspectives around Korea cla code usage? And I'm going to stick with research because it will think hard. Um, and I can just tell it to go. And that's an example of how you can start to really kind of dive in and get farther. Now, one of the beautiful things about Perplexity is how flexible it is. So, while this is working, I can show you other ways to use Perplexity that are super useful. So, for example, we can choose, I know I promised to show you, we can choose to move this to academic or social or finance. We can choose to connect to other sources. So, it will search across these other sources. We can upload a file here if we want to or a Google Drive. We can speak our search. We can also get into finance and there's a whole finance product that's been built. Uh we can get into spaces. We can get into discovery for sports and culture. I think most people do not realize how effectively perplexity is owning the rich experience on the web. In the meantime, I want to talk about how we avoid hallucinations with perplexity because I get that question a lot. If it's the internet, how do we talk about avoiding hallucinations? Number one, never trust single source answers. Perplexity will site AI generated spam because it cannot tell the difference between an AI generated source and a real source. And sometimes the AI generated source is correct and sometimes it's wrong. But perplexity can't tell either way. If perplexity is only citing one source and it's an unfamiliar blog or a random LinkedIn post, treat it with skepticism. You want to be in a position where you can verify the claim with a wellsourced article for a real publication of some sort. I would also suggest if you are interested in authoritative sources, which you should be if you're using perplexity, use another LLM as a tool. So, I think I'm going to build a cross-checking hallucination prompt intended for chat GPT or Claude to go with this post because I want you to have tools to basically say, "Here's a perplexity search result. I'm not sure I believe it. Let's go to an LLM and ask the LLM to do thinking critical thinking on the post and also internet searching so that I can get a second perspective here because that two tool verification loops do work and you can use chat GPT to check perplexity's work and you can also use perplexity to check chat GPT's work. I've done that both ways. One of the things you have to be especially careful about is how perplexity attributes quotes. So, perplexity describes a quote. Please make sure you go to the cited source and search for the phrase. It is often there, but it may not be there verbatim. It may be in a different format, and it may not have the connotation in context that perplexity is suggesting in its synthesis. You have to be careful. Finally, if you have very precision critical queries, I would encourage you to select academic focus, which prioritizes peer-reviewed sources like PubMed or Semantic Scholar, and that is because that reduces the probability that you're going to get AI generated spam that's in the rag architecture that perplexity can access that sort of creeps in to the answer set. If it's focusing on academic peer-reviewed journals, it's less likely to get stuck in AI slop. The reality is hallucination is absolutely an issue with perplexity. If you ask it for verified links and you go back and check the verified links, many of them will work, but not all of them. And so there is really no substitute for that double LLM check. And finally, for you as a human owning the results. Last but not least, I want to leave you with a few thoughts on why why we use perplexity doll. Why does something like this matter in a world where we have Google and we have Chad GPT? Isn't this just the awkward in between space? The answer is no. I think perplexity is relevant because of the knowledge recency problem. LLM training data gets out of date too fast. AI knowledge is adding to our understanding of the world very quickly. Humans are writing very quickly on the internet despite the issues with hallucination and the risks with searching on the open internet. There is no substitute if you want recent information. You can actually update a rag knowledge base like perplexity has multiple times a day and perplexity has gotten much better at that in the last few months. Whereas Chad GPT treats current information as not a part of its core parametric model. That's one of the fundamental limitations of current large language models. It does not update. But perplexity it's like you can update the foundation every day. The other thing that I think really matters as we talk about hallucinations and the importance of good information in the age of AI perplexity may not be perfect but it has an accountability architecture. Rag allows you to create verifiable chains of reasoning through transparent sourcing and everything you see on perplexity is sourced and you may disagree with the source. You may have concerns about the source but you can see it. That is not always true with LLMs and that's a big deal. Finally, I want to call out that this gets a tiny bit philosophical, but stick with me. Chat GPT and perplexity have different epistemological architectures. Big words, but really what it means is LLMs will excel in cognitive intelligence like reasoning and language generation. And a rag architecture is actually focused more on fetching facts and doing so precisely. So, chat GPT will say, I believe this is true based on patterns. That is one of the roots of hallucination in LLMs. They want to be helpful. They have parametric patterns in their data and they just do that instead of searching or using tools. Perplexity says these sources claim this. I found the sources. Here are the sources. You figure it out. As LLM get better at sounding confident, we need something like perplexity more because the gap between fluency and factuality widens. Shad GPT sounds more and more fluent, but it may not be factual and we may not be able to tell. So, I think that sounds philosophical, but I think perplexity occupies a really important place culturally as AI continues to get smarter because it allows us to actually have an AI native approach to looking at facts, not just patterns. And I think that's a really big deal. Let's go back and check on our perplexity search. Here we are. I never would have found this. I did not plan this. I'm discovering Korea's clawed code culture. I get lots of facts on this and I can see at a glance that they're useful, right? Anthropic is a reputable source. I can go through, I can see Reuters. This looks like a pretty well sourced approach. I'd have to dig in, but like it looks super interesting. I'm looking at uh the interaction between Korea's work culture and claude code and how that works. This is a super fascinating example of something that you would never ever ever get to in JPT. I could not have gotten this report no matter how good my prompting was because this report depends so heavily on finding facts on the internet. And this is why perplexity is such a joy to use and why I use it so much. It's just fantastic for discovering corners of the world that you didn't expect. I hope that this has helped you to understand why perplexity matters, why we should have it. I'll capture up those nonobvious prompting techniques. I'll suggest some specific starter prompts for you. My goal here is for you to feel like the world is your oyster with perplexity and to have a sense of how important it is and how you can use it to be more effective in your search. It is not at all the same as chat GPT search and I hope that you can see that. Best of luck with uh search in the
