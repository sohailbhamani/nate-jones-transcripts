---
title: "Trump Just Gutted State Al Laws, OpenAl Panicked, Al Agents Stole $4.6M--Your 10-Minute Breakdown"
video_id: "pEsoqm0o3Dk"
youtube_url: "https://www.youtube.com/watch?v=pEsoqm0o3Dk"
publish_date: "2025-12-13"
duration: "12:03"
duration_seconds: 723
view_count: 22731
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  - "AI strategy"
  - "GPT-5"
  - "OpenAI"
  - "Gemini"
  - "AI agents"
  - "LLMs"
  - "AGI debate"
  - "AI regulation"
  - "frontier models"
  - "humanoid robots"
  - "Figure AI"
  - "Nvidia export ban"
  - "DeepSeek"
  - "Andrej Karpathy"
  - "AI news weekly"
  - "future of work"
  - "AI compute scaling"



# AI-enriched metadata
content_type: "News Roundup"
primary_topic: "AI Agents"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
  - "Founders"
entities:
  companies:
    - "OpenAI"
    - "Google"
    - "Meta"
    - "Amazon"
    - "Nvidia"
    - "Intel"
    - "AMD"
  people:
    []
  products:
    - "Gemini"
    - "Make"
  models:
    - "Gemini"
    - "Gemini 3"
    - "DeepSeek"
concepts:
  - "That space vents heat and so we should be able to put our hot data centers in space and beam the tokens down via lasers"
summary:
  - "# Trump Just Gutted State Al Laws, OpenAl Panicked, Al Agents Stole $4"
keywords:
  - "ai-agents"
  - "ai-news"
  - "ai-strategy"
  - "amazon"
  - "amd"
  - "coding"
  - "deep-dives"
  - "frameworks"
  - "gemini"
  - "google"
  - "intel"
  - "leadership"
  - "make"
  - "meta"
  - "nvidia"
  - "openai"
  - "product-management"
  - "workflows"
---

# Trump Just Gutted State Al Laws, OpenAl Panicked, Al Agents Stole $4.6M--Your 10-Minute Breakdown

I spent more than 20 hours this week tracking AI news and I'm going to give you the eight stories that matter in just the next 10 or so minutes. Number one is chat GPT 5.2. Chat GPT 5.2 launched from OpenAI after an internal code red initiative to rush the model out following Gemini 3's launch and takeover of leading AI benchmarks. It was an absolute scramble by OpenAI to accelerate timelines. The updated system card from 5.2 to highlights new controllability features for style, for tone, and even for safety behaviors that are designed to address enterprise compliance requirements and regulatory scrutiny around frontier models. The 400,000 token window is a big win and represents a really significant gain in terms of the model's ability to do real hard workflows over time. So, you can throw a 300 plus page research document in there and get real analysis across that entire surface. The competitive pressure is really evident here. OpenAI's release cadence has accelerated from every say 6 months between major GPT5 updates to just a few weeks between 5.1 and 5.2 and they are rumored to be coming out with 5.3 or something similar in January. So, the pace is going to keep accelerating. What to watch for here? Keep an eye on the API pricing for 5.2. Expect it to be significantly cheaper than 5.1. and keep an eye on how long they keep their code red scramble. Open AAI needs to be leading benchmarks to continue to fund raise. Story number two, the Trump administration signed an executive order aiming to preempt state AI regulation and threaten federal funding. So the executive order is entitled ensuring a national policy framework for AI and it directs the federal government to establish a single much lighter touch AI regulatory standard and to actually actively block state level AI laws that are deemed inconsistent with national competitiveness goals. The White House framed the order as necessary to prevent a patchwork of 50 different AI compliance regimes that would hinder US AI company's ability to compete globally. Think of this as being framed around the larger great powers competition narrative that is popular in Washington where Washington wants to promulgate an AI competitive landscape that will enable companies based in the United States to compete effectively with companies based in China. Keep an eye out for which state laws the Department of Justice targets first here. It might be California's SB 1047 revision. It might be Colorado's bias audit, which is a requirement they recently launched. And also keep an eye out for how many states push back with legal challenges. Congress might introduce federal AI legislation in 2026. I would not hold my breath. Until then, keep an eye on how the EU and other jurisdictions allocate market access to US AI systems if they remain concerned about AI accountability frameworks in the US. Story number three comes from Tim Demer, a researcher at the Allen Institute for AI and a former CMU PhD known for pioneering GPU quantization techniques, including the famous Qura technique. He wrote a widely discussed blog post this week entitled why AGI will not happen because he argued that meaningful GPU performance has already peaked and that both AGI and super intelligence are fantasies rooted in ignoring the physical constraints of computation. Atm argues that GPO GPUs have added one-off features since 2018. He argues those features are now exhausted and further improvements face diminishing returns. His core thesis rests on three claims. First, GPUs maxed out performance per cost around 2018 with subsequent gains from architectural tricks. Second, transformers are near physically optimal for balancing local computation and global information pooling or attention. And third, scaling laws require exponential resources for linear improvements, meaning the industry has maybe one or two years of scaling left before infrastructure costs outpace capability gains. The post triggered significant debate across the AI research and skeptic communities. Some folks were amplifying Detmer's arguments on less wrong and tech press outlets like the Silicon Florist. Others were arguing that the perspective that he brings while credible because he's been involved in GPU optimization before is too at odds with where hardware vendors like Nvidia, AMD, and Intel see things going to be ultimately correct. My take on this is that Demmer's is probably correct about the details and incorrect about the larger stories. I think he clearly has the expertise on understanding how GPUs work. I think what he misses is that the way we make compute scaling work for the last 50, 60, 70 years, the way we made Moore's law a reality was not by following one technical trick over and over. It was an entire industry allocating capital and attention to focus on driving compute forward. That is exactly the same dynamic we see here in AI. And that is why I think it's incorrect to lock AGI to a particular technical breakthrough that should happen on a GPU. We may be limited on GPUs for some reason, but there is enough capital and attention going into innovation that I do not see a long-term limit on our ability to scale compute. And for the record, neither do any of the researchers at the major labs. If you're working at OpenAI, at Enthropic, at Meta, if you're working at Google, you don't hear people talking about a wall. In fact, they are aggressively saying, "We don't see a wall. We see continued scaling." And they're not just saying it, they're shipping it. So I think we have to look at the empirical evidence, see the continued scaling, and ask ourselves, why wouldn't this keep going? The default stance should be that we'll continue to allocate capital. We'll continue to allocate attention, and we'll continue to see scaling breakthroughs. Maybe not with the specific techniques that Demmer's called out, but overall, we should continue to see improvements in intelligence. What if I'm wrong? But if Demmer's is right, ironically, even if Demmer's is correct, we are still set in for more than two decades, I think, of AIdriven corporate disruption because the existing capability set that is already baked in is already so disruptive that an entire generation is going to have to spend their careers working AI into these systems. Story number four, anthropics. AI agents exploited smart contracts for more than $4.6 $6 million in simulated theft. The story here is the continued gain in autonomous AI agent capability. The agents were only given contract addresses and very highlevel instructions like find and exploit vulnerabilities. And they showed that they could autonomously perform reconnaissance, craft exploits, and validate attacks. The lesson here is pretty simple. Agents are going to keep getting better and we are going to keep seeing new exploitations and IT security professionals need to assume that any agent out there in the wild is a potential hostile. Story number five, Andre Carpathy went viral with a thread arguing that LLMs are simulators of perspective and we should not use pronouns like you when talking with LLMs because it pushes them toward an averaged midbasin opinion that is not really reflective of any internal sense of identity from the LLM. it's just reflective of sort of the averaged out pre-training data that they get and that you can get much more interesting responses by actually asking the LLM to act as a simulator within a particular role like a researcher, a product manager, a CTO. The irony here is that everything comes full circle, right? We were just done announcing to the world that roles are done and now we're back to saying roles matter. I want to call out that if you all have a good understanding and a good mental model of LLMs, you are not going to be taken for a ride when people change their opinions or when new things like this come out. I have been saying for a while that roles are not useful perhaps for accuracy of answers, but they're useful for steering LLMs through highdimensional latent space toward particular attention syncs where we can get useful next token predictions. Basically, that's what Andre Karpathy said, but he's smarter and and he absolutely said it correctly and I'm glad it's getting attention because I think that there has been too much attention around just don't use roles and use these magic words instead for prompting. We need to think about the broader capabilities of these models and Andre has been a leading light on challenging assumptions and I think in this case he's challenging anthropomorphism. He's saying don't treat the model like a people, treat the model like a simulator and allow it to simulate different perspectives. That's a really useful take. Story number six. Elon is proposing orbital AI compute. The story here is brief and simple. It was a debate. Nothing is in space today. The core idea is that space vents heat and so we should be able to put our hot data centers in space and beam the tokens down via lasers. Someone is going to try this within the next year or two. We are going to see someone attempt a data center in space powered by solar panels and we will rapidly see whether this is a plausible way to scale our data center footprint and scale LLMs or not. My take here is strictly empirical. We don't know if it works. It's not clear that we can actually do this. If we can do it, someone's going to try it and find out. Story number seven. Deepseek is reportedly using smuggled Nvidia black weld chips despite export bans. The information reported this on December 10th. A Chinese AI startup known for costefficient models has been using GB200s and B200s which are banned from export to China under US semiconductor restrictions. Nvidia has said they have no record of a phantom data center, but they cannot they cannot prove that Blackwell samples have not gone missing from legitimate buyers in Southeast Asia in the Middle East, suggesting there may be some leakage in GPU networks that could end up in China. Guys, this is my surprised face. Story number eight is humanoid robots shifting from lab demos into practical deployment. UBS actually got in on this, forecasting 2 million workplace units for robotics by 2035. Ironically, I suspect the fact that they made that projection suggest that the reality will be much higher. Multiple sources over the last week have highlighted accelerating progress in humanoid robots with Figure AI's humanoid transitioning from stiff limited walking in 2023 to dynamic balance correction to sorting packages in 2025. It's been a roughly 18-month development cycle and figures already deploying their robots to factories. By the way, Mmer's called out that he doesn't see a case for robotics. And I think it's really, really interesting to see the same week that he says that we see an hourong video posted by the CEO of Figure AI. Basically, all it shows is a humanoid robot standing on an assembly line correctly sorting packages. Robots are coming. The idea that robots are not economical to deliver is just fundamentally incorrect. We are going to see continued decline in costs per unit and I would expect to see declines below $10,000 in the next few years. Keep an eye on enterprise deployment patterns next year, especially if Amazon, BMW, Foxcon or others start to scale in humanoid robots. That is going to be a tipping point for the industry. Also keep an eye for whether household task performance improves enough to justify a consumer purchase tipping point for say the 2027 holiday season. Current capabilities still require really significant human supervision in the household. And it's not clear yet whether we have cracked the code for the varated tasks that would enable a robot to be a true household helper. And that's all eight stories. Tell me what I missed. Tell me where I'm wrong. Tell me what you think is going to happen next
