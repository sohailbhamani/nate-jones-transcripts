---
title: "OpenAI's Secret Agent Builder Just Leaked (First Look + Why It Changes Everything)"
video_id: "vy9pQe-lYDE"
youtube_url: "https://www.youtube.com/watch?v=vy9pQe-lYDE"
substack_url: null
publish_date: "2025-10-06"
duration: "15:40"
duration_seconds: 940
view_count: 16042
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  - "AI strategy"
  - "AI agents"
  - "LLMs"
  - "automation at work"
  - "future of work"
  - "prompt engineering"
  - "OpenAI"
  - "GPT-5"
  - "Model Context Protocol"
  - "MCP tools"
  - "Agent Builder"
  - "prompt injection"
  - "guardrails"
  - "token burn"
  - "multi-agent chains"
  - "deterministic agents"
  - "organizational standards"
  - "AI strategy for teams"
  - "AI jobs for builders"



# AI-enriched metadata
content_type: "Tutorial"
primary_topic: "AI Agents"
difficulty: "Beginner"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "OpenAI"
    - "Google"
    - "Box"
  people:
    []
  products:
    - "Claude"
    - "Copilot"
    - "Make"
    - "MCP"
    - "Model Context Protocol"
  models:
    []
concepts:
  []
summary:
  - "# OpenAI's Secret Agent Builder Just Leaked (First Look + Why It Changes Everything)

Shots have been fired in the agent wars"
keywords:
  - "ai-agents"
  - "ai-news"
  - "ai-strategy"
  - "ai-tools"
  - "anthropic"
  - "box"
  - "career"
  - "claude"
  - "coding"
  - "copilot"
  - "frameworks"
  - "google"
  - "make"
  - "mcp"
  - "microsoft"
  - "model-context-protocol"
  - "openai"
  - "product-management"
  - "prompting"
  - "tutorials"
  - "workflows"
---

# OpenAI's Secret Agent Builder Just Leaked (First Look + Why It Changes Everything)

Shots have been fired in the agent wars. OpenAI is launching their new agent builder experience and I want to tell you all about it and also give you the tea on my own experience building with agents because it's about to become everybody's job. So first, what is OpenAI launching and why should we care? OpenAI is launching a drag and drop user interface agent builder. Think of it as I drag the little Lego bricks and tiles along and I can see really clearly what my agent will look like because first it ingests a Google doc here and then I tell it to decide with chat GPT here and then it comes out and goes into a spreadsheet over here. That's a simplified example. And I can connect them with arrows and I can define the logic. And apparently chat GPT is adding special hardening that is designed to be appealing to companies like prompt injection protection, like guard rails against not safe for work language and other protections that right now are limited to companies that can afford to install them custom and are not easy to get out of the box. So, one of the things that chat GPT of course wants to do is to push people into using their own chat experience more and more, right? Like that's just makes sense. An agent builder like this with builtin protections that corporations care about is designed to pull all of the casual agent building into the fold, right? Into, hey, we can use it with chat GPT. Why would we go to Copilot? Why would we go to Claude? Why not just do it in chat GPT? because it's so much simpler to pass it security review. That's what's in their heads and that makes people feel safe and if people feel safe building, they're going to build more and it becomes a virtuous feedback loop. So that's the strategy. That's the thinking. Let's talk about how this actually works. Because one of the things most people don't realize is that there is a giant gulf between casually designing an agent as a fun little weekend project and designing an agent that has to work in production. And one of the things I've been advocating for as someone who has seen, shepherded, helped build agents in production at large companies is that we have to bring that big company thinking down in a format that's recognizable and easy to understand to a point where teams and individuals can use it successfully and take those principles and apply them at their own scale. And that's what I want to do with the rest of this video because you are all about to get tremendous agent building power. NAND may have felt like a foreign territory. You may not have had the ability to build agents yourself or feel like you want to go to a different tool. Almost everybody uses chat GPT somewhere and chat GPT is about to become a place to build agents. Hundreds of millions of people are going to have agent building powers for the first time. What do you do with those powers? Let me give you my hard one scars on experience for how to build agents. The first thing to think about with your agent use case, it's funny. Is it worth it? And I say that because a lot of times people have this funny radar when they start with agents where they pick the use case that isn't worth it. I have seen over and over that people think, well, this is new. This is experimental. I don't want to wreck anything. Let me try something that isn't too serious. That's a problem. And that's a problem because you won't take it seriously. The rest of the org won't take it seriously. and you're not really going to have the time and energy to prioritize it in a work context. And if you do, you're not going to care about whether it worked or not. So, please pick a problem that matters. Have some courage. Pick a problem that you actually really would like some magenta help on. That's my first hard one tip for you. Number two, think obsessively about the outcome and also how you know it's right. Those are two things that people often miss. They often start building from the from the beginning of the agent thread like so what is the agent going to trigger on right what is the input for the agent those are really important questions but I'm just going to tell you from a principal's perspective successful agent builds start with designing for the outcome they start with designing for what you want it to be done and then as an additional layer how can you prove it how can you know it was done right and that has different levels depending on your work right like you might be in a place with marketing copy where it's like we can look at it, we can feed the text to another LLM, it can verify the grade level of the reading, it can do a quick fact check and we're done. Or it might be a production workflow for a office operation and you have to have the health information correctly categorized. Well, now the checks are much higher. You have to keep a record of every run. You have to be able to prove that it's stored securely and you have to be able to make sure that you are actually building it correctly from the start and that every single run works. So think about the stakes, think about what correctness looks like, think about the outcomes, and then work backwards from there into the design. Because one of the things that you will realize really quickly if you adopt that outcome first, prove it first mindset is that you get really, really stubborn, this is going to be so ironic, but this is tip number three. You get really stubborn about picking the dumbest agent you can. I'm not kidding. Do you know why you get stubborn about picking the dumbest agent you can? Because in my experience across lots of agent builds, the dumb agents work better if they are fed the right context obsessively. Basically, what you are trying to get to is what I would call deterministic intelligence for companies. And until we get a future solution that truly is thinking intelligence without any risk of hallucination or anything else, you are going to need to make sure that you have predictability. And by the way, hallucinations in a business context are a lot more than just making something up. So one of the guardrails that OpenAI is planning on launching is around hallucinations. And that's great. But it is the egregious hallucinations that are covered as opposed to the followed the process correctly but made a different choice here and the prompt was ambiguous and so I could have made either choice and I made this B instead of A. That is not a hallucination. It might be treated that way but it's not. It's not guardrailed. It's on you to design it appropriately. And the way you avoid those kinds of business logic mistakes is by dumbing everything down. Your prompt needs to have zero ambiguity in it. It needs to be crystal clear. Your data sources need to be extremely structured, extremely organized. And the model in my experience works better in that context if it's just a simple dumb rule following model like go to GPT5 and turn the juice down, right? no reasoning power and then just let it run because you would rather be in a position if you're designing an agentic system where you have multiple dumb nodes, multiple dumb agents doing individual tasks in your flow versus one super smart agent that's supposed to do it all. Because the super smart agent that's supposed to do it all, are they going to have the auditability? They're not. Are they going to be able to show you how they did the work? No. Are they going to have some ambiguity that just comes from doing the whole task at once? Yes, they are. And so you would rather decompose the task into a bunch of individual steps and pick dumbish agents to do those steps. Basically, the minimum intelligence needed to do the steps. So you can troubleshoot it. So you can audit each step so you can understand what each step is doing very specifically. So you can design the context appropriately for every single step. Is that more work? Yes. This is why we're having this conversation, guys, because most people are going to try and juice up the power on their AI models and do everything in one step. And they're going to be like, "Oh my god, why am I not getting predictable results? Why is why is my thinking LLM going off the rails?" And then you're going to look and the context window is stuffed and they don't have a clear prompt and it's prompting an ambiguous prompt with a stuffed context window and no clear guard rails on what it finishes with or what it does or what A versus B is. Yeah, of course it's not going to go well. But that sure will seem convenient, right? Like juice up the power and just stick one node in there and fix it with your agent. It's not going to work. Also, incidentally, it's a big token burn. I am not sure quite how chat GPT5 is going to measure token burn for these repeated jobs. That remains to be seen. But I will tell you, you want to be thinking about token burn now because you are going to be in a world where it matters sooner or later. Agentic systems aren't free. They do the same job over and over again. If it's marketing copy, maybe you want a 100 blog posts a week. If it's health records, maybe you need a thousand done a day. But whatever it is, it gets done at volume. And so if your context is fat, if your prompt is ambiguous and burns token to parse, if you have too many choices to choose from, it's all going to confuse the model and you're going to pay for it in tokens. This brings me to tool choice. You need to be really really clear with your MCPs and your tool choice. This looks like it's going to be the most widely available release of model context protocol servers out there. Chad GPT's footprint is bigger than anybody else's and they say they are launching with MCPs as the connection points for tool calls for these agents and it's going to be drag and drop and super simple. Well, welcome to MCP everybody. The way to do this properly is to make sure that your agent has a clean dictionary of tools that it can use within its world. And if those tools are MCPS, that's fine, but it needs to know what each one is for and under what conditions it calls them. You should not leave the LLM to make the judgment call of which tool to use without guidance. it can choose which tool to use with guidance from you. And that's really important because you're essentially going to need to compose a prompt for the LLM based on the retrieval context it has, the inputs you're giving it, any system instructions or prompt that you have for it, and then whatever tool use that it uses, right? And so it's basically going to go through, it's going to read the retrieval, it's going to read the prompt, it's going to select a tool during the run, and then it's going to come back with a response and put it wherever you want it to go. It is dependent on your clarity in your prompt and in the retrieval to know what tool to use. If there is ambiguity, you will get unpredictable responses. And so my recommendation is if you are in pointand-click land with this new builder and you're super excited and you want to design all your tools and someone on the internet said, "Look at my 20 MCP server tools. Aren't they cool?" Just pick the simplest, smallest collection of specific tools that do one job and put those in as MCP servers. If you bloat out your tool catalog too fast, it's kind of like giving a seven-year-old access to a bunch of power tools in a wood shop, you should not trust them with that choice. You should give them the tools that are appropriate to what they can do. And that is what you need to be doing. You need to think in in each call, in each agent that you set up, what are the appropriate tool choices? How does the model disambiguate and clearly pick between these tools? And if it picks a particular tool, can you come back and see that it ran it successfully? And that is where having multiple local LLMs in a chain that are relatively dumb is helpful because you can see the responses and run the trace and actually see, ah, look at that. You know, node number two really screwed up here with the MCP tool call. You're going to want that. You're going to want that. I should call this my survival kit for agent building because that's what it feels like. This is the stuff that I wish I knew going in. One more thing that I want to call out. When you are designing these systems, you are going to be tempted to bite off more than you can chew. And I realize that I am saying that as I just told you at the beginning of this video to please pick a goal that has real stakes, that has real meaning. I did say that. That's true. You should. But there's a difference between picking one goal that has meaningful stakes for your first agent build and doing it well and trying to bite off 800 tasks to solve across the business. Please just focus focus on one thing that matters and then expand methodically because one of the things that is not at all clear about this release to me and that really organizations are going to have to work out is what are the best practices for agent builds that organizations want to insist on for their teams and how can you socialize those out. It's going to be much more complicated than custom GPTs. Custom GPTs are already kind of a mess in organizations. Imagine a world where everyone is messing around with different rules and conventions for prompts. And it's not just the engineering team now. It's everybody because everybody has this point andclick interface and they're doing production workflows, but they're sitting in little sort of custom aentic sort of little workflows that only the marketing team knows about or only the product team knows about and you can't manage them. And you have no idea what happens when Betty goes on vacation because she's the one that put the workflow together. It doesn't work. And you also have no idea which MCP servers are being touched by which agents in your in your environment. You just have no clue. So there's a lot of unanswered questions there. And I think one of the things that I want to challenge you with is you can answer those proactively by having an organizational response by saying as a team, as an organization, these are our standards for agent builds. This is what we care about. We care that you pick the dumbest possible agent for the task. We care that you define the simplest possible workflow that will get the job done. We care that you define the cleanest possible context for your given task. We care that you pick the fewest, dumbest, most specific, and clearly differentiated tool collection. We care that you have a tool dictionary. We care that your prompt has been vetted so it isn't ambiguous. People load the prompts with adjectives. People load the prompts with multiple meanings and they wonder why is their token burned? Why does the agent not behave predictably? I've got news for you guys. The agents aren't magic. They they are trying to parse your ambiguous human language. Give them more structured instruction with less ambiguity and you will get better results. So that's my plea to you. You You are all about to have kind of like Luke Skywalker the ability to build your own lightsaber which is super cool. But please be careful to build it right. Please be careful because the consequences are an insecure agent that generates production workloads that nobody has monitored or nobody has watched over, nobody's able to maintain when you're out and that generates ultimately organizational vulnerabilities. And as much as Chad GPT is going to lean on the safety guard rails, which are cool, it's not enough. It's teams jobs to design agentic policies for teams that work for the whole team, not just the individual. And as an individual, it is your job to build the most scalable and sustainable agent you can. And that is what these principles are designed to do. Good luck with all the power you're about to be given. It is a really cool world. I've seen agents do amazing things. Don't think that I'm negative on them. I love them. But boy, do you need to think about how you design them. I've put together a prompt if you'd like to sort of dive into it over on the Substack to help you have the conversation around these best principles, these the best practices principles, and also to think about your own unique context and put together an agent architecture that works for you. So, if that's something you're interested in, great. Have fun with it. I hope it helps you design solid agents that are less likely to break. Have fun and uh happy watch
