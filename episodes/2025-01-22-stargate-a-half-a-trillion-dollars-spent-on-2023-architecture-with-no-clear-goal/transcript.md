---
title: "Stargate: a half a trillion dollars spent on 2023 architecture with no clear goals?"
video_id: "L4TT6OAtuS0"
youtube_url: "https://www.youtube.com/watch?v=L4TT6OAtuS0"
substack_url: null
publish_date: "2025-01-22"
duration: "6:18"
duration_seconds: 378
view_count: 3806
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "Tutorial"
primary_topic: "AI Strategy"
difficulty: "Advanced"
audience:
  - "Product Managers"
entities:
  companies:
    - "Anthropic"
    - "Google"
    - "Meta"
    - "Microsoft"
    - "Nvidia"
    - "X"
    - "Oracle"
  people:
    - "Sam Altman"
  products:
    - "Gemini"
    - "Make"
    - "Projects"
  models:
    - "Gemini"
    - "SAM"
concepts:
  []
summary:
  - "# Stargate: a half a trillion dollars spent on 2023 architecture with no clear goals"
keywords:
  - "ai-news"
  - "anthropic"
  - "frameworks"
  - "gemini"
  - "google"
  - "make"
  - "meta"
  - "microsoft"
  - "nvidia"
  - "openai"
  - "oracle"
  - "product-management"
  - "projects"
  - "x"
---

# Stargate: a half a trillion dollars spent on 2023 architecture with no clear goals?

Stargate is out it is both a pretty terrible TV show for the 1990s and also a half a trillion dollar infrastructure program that was just announced all about AI I have some real questions the issue with Stargate as far as I can tell is that it crowns a winner before the race is over it says open AI is going to win the game they're going to win it funded by Soft bank and Oracle is going to build a data centers obviously for those three players that's great even Microsoft gets in on the game they're happy to they're partner of open AI Nvidia of course is supplying the chips the problem is that there are a lot of other players in the game and it is not clear how this reshapes the race for them they're not giving up anthropic is not giving up I know I just did a video on them but they're not giving up and meta's not giving up Google's not giving up the Stakes are too high and yet here we are crowning a winner and I'm not even getting to the shifts that we've seen with model makers like deep seek entering the scene or how x. a is coming on quickly with Incredible gains uh huge compute clusters and so when I look at the problem space and I say to myself this is a hugely Dynamic situation there's lots of model makers they're all competing how does it make sense to have only one model maker get in on this project I don't think it does and I think it exists that way because this was Sam Altman shopping a deal for this kind of a data center back it feels like almost a year ago like it was like 10 11 months ago and then it died down and now it's back so that brings me to my second issue with this this is a 2023 architecture that they are describing not a 2025 architecture and I don't know why that they are like what that doesn't make sense we've learned so much this is such a dynamic space it changes so fast so I'll explain what I mean 2023 we thought that we had to have ever bigger clusters of gpus to train on ever bigger data sets in order to make these mod smarter we thought that in early 2024 too that's why this thing is talked about as having 10 million gpus well the thing we discovered is that at the end of the day you can have all of that compute but there may be diminishing marginal returns just for pre-training you have issues finding the data unless you're generating it synthetically which we've made some progress on but that's a big scale up in synthetic data production if that's what we do as as Ilia famously said in November of last year long after Stargate was first kind of kicked around we have one internet right like we have one internet siiz data pool we've used it so I think the reason why that feels weird in that context is that this is a architecture that fundamentally assumes this sort of older Paradigm for how trained AI models and the new paradigm the one that's unlocking continual progress that everyone's excited about it's not mentioned inference time compute is a very different Paradigm it allows you to run multiple threads simultaneously is what happens when the model thinks frankly Gemini dropped a version of that yesterday with their new update to flash 2.0 thinking it apparently I haven't had a chance to even try it yet apparently it's on par with o Pro so the model makers are continuing to compete they're competing on different architectural standards and Stargate is sitting here like with this 2023 structure and everyone's saying it's going to be a you know vaccine for cancer this and that well maybe but it's a weird way to go about it now and it makes me wonder if we've seen this much drift in the way we do AI in a year because we're learning so much and this thing takes four years is this just going to feel outdated by the time we're done with it it might it might and that kind of comes back to the goaling like in other major infrastructure projects that America has undertaken we've had very clear goaling you go to the Moon you bring back the astronauts it's super clear by the end of the decade they even had like a classic timeline on it fine this is not very clear it's like yeah we'll do some answer stuff okay what what does done look like what does good look like does this mean that like also the defense department will be using it maybe it's not really clear who gets to decide how all of that compute resource is allocated that's also not clear does soft Bank decide that I doubt it so I have a lot of questions as you can probably tell I think it absolutely reshapes the race it's worth talking about I put more thoughts on my substack but at the end of the day to me this is a project that makes me tilt my head and think and raises more questions that it answers and everyone's sort of talking about it as if it's a done deal it's obvious like this is it I I don't know like I don't think building the future on two years ago architecture four years from now is automatically the win maybe like maybe they just repurpose the compute I don't know but it feels a little odd what do you think
