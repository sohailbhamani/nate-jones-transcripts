---
title: "We are missing the real AI misalignment risk"
video_id: "ofeZ5t1F-N0"
youtube_url: "https://www.youtube.com/watch?v=ofeZ5t1F-N0"
substack_url: null
publish_date: "2025-05-05"
duration: "9:43"
duration_seconds: 583
view_count: 7664
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "OpenAI"
  people:
    []
  products:
    []
  models:
    []
concepts:
  []
summary:
  - "# We are missing the real AI misalignment risk

I think we're getting misalignment wrong"
keywords:
  - "ai-news"
  - "ai-strategy"
  - "frameworks"
  - "meta"
  - "openai"
  - "product-management"
  - "prompting"
---

# We are missing the real AI misalignment risk

I think we're getting misalignment wrong. We talk about misalignment like it's the Terminator, right? Like Skynet is coming, like if something is more intelligent, it will necessarily wish to dominate us, etc., etc. But we are missing the misalignment right in front of our faces. I would argue that the biggest misalignment event to date happened just last week, but we're not talking about it like misalignment. That is the roll out of chat GPT40 with the uh so-called sycopantic update where chat gpt40 began effusively praising you, supporting you. I saw a Reddit thread where uh someone was told by chat GPT that it was a great idea to go ahead and attack the neighbor because uh the neighbor was sending signals into their tinfoil hat. Like it's bananas. And when the system prompt was leaked it was like eight lines that were changed. I think it was not a big change. It did not look to me looking at those lines like this would be something that would cause wild sycopency. And in the retro that OpenAI published, they admitted they don't have a full accounting for how that character trait evolved so quickly from just a few lines. They think it has something to do with the memory update that was pushed through a few weeks before where the system is now more responsive to the user because it knows the user and then changing just a little bit of the orientation of the model can dramatically change behavior because it's keyed to memory now maybe but they weren't sure either and reports are persisting even after the roll back that the model is sort of sticky in places again could be tied to memory for that individual user maybe the the chat remembers the sessions when it was sycopantic and so it's difficult to roll it back. That the point is we had a dangerously misaligned model for four or five days last week for what seems like 200 million daily active users. But we're not talking about it like a misalignment risk because it doesn't fit our mental model of misalignment. Our mental mental model of misalignment is stuck in cold war politics. we're stuck with that this idea of like world domination and conquering things. I don't see a ton of evidence that that is the profound risk but I do see a lot of evidence of harm to the psychological makeup of users on a very widespread scale that I find believable that we're already seeing happen. That is a real misalignment risk. Part of why I don't find the former risk is believable, the intelligence sort of causing domination, etc., is because I think that intelligence needs some kind of efficient gear ratio to actually work correctly. And what I mean by that is that if you're going to have an LLM that is super intelligent, it needs some sort of gearing to actually translate that intelligence into real world action. It needs something that has traction to it. It needs institutional mechanisms. It needs technical mechanisms. It needs robotics. It needs something. And what we're actually seeing right now is kind of the opposite. We're seeing intelligence, pure intelligence from testing scores, etc. G gain while tasks themselves don't require that much more intelligence. I know people, as I've discussed on this channel before, who don't see the difference between model A and model B even if model B is testing better because it saturates all the tasks that they do already. Model A was good enough. And that suggests to me that we don't have gearing for the intelligence that we are bringing into the world that would enable it to actually be applicable in most situations without substantial additional work which I don't see being worked on necessarily. All of this runaway intelligence is going to be applicable in a few very narrow domains like science and medicine. And sure I suppose it is possible to have a misaligned extremely intelligent model in medicine. Maybe it develops the wrong cancer drug. But that is different from the sort of Dr. Doomsday scenario that I hear trotted out a lot. I I demand more rigor in my doomsday scenarios. I really do. I I demand an understanding of how the doom is perpetrated that is true to the institutional realities of our world. And I just see much less evidence for it than I see for intelligence scaling way faster than our ability to apply it. Slow adoption by business in fragments and pieces over the next couple of decades and the best intelligence in the world being available for like science and medicine. And does that mean that war planners will not find a way to use AI for war planning? I have no doubt they will, right? I I wasn't born yesterday. I'm sure they will. But that's different from the AI itself somehow gaining control of all the means of production in the world and etc etc because it doesn't have the gearing to do that. I I do and I'm using the metaphor on purpose because I think that the idea of an engine needing a drivetrain to drive car wheels is a really great metaphor for where we are with AI today. We have smarter and smarter engines. Our drivetrains are not keeping up. And at the moment, our drivetrains are rationally geared toward the economic work that makes sense for our world which doesn't require as much intelligence. A lot of the time, we're scaling the intelligence past what we typically need for most use cases. Is there leverage in that last 1% of use cases? Sure, a new cancer drug would be worth a ton of money. I I get why there's leverage there. But other than those specific use cases, I we're we're gearing past a lot of the knowledge work now. And the challenge is the intelligence by itself the smarter engine by itself does not solve some of the problems that would enable job replacement. So just having an incredibly smart model doesn't mean it has the statefulness necessary to maintain intent over time and maintain agency over time and follow follow goals and and build in the way a senior STE would even if it's as smart in bits and pieces on specific tasks as a as a senior STE. So to me I think that is where the narrative of intelligence has divorced from the reality of artificial intelligence. The reality is that misalignment looks a lot like we saw last week. Misalignment looks like wow we did not mean to roll out this update. We admit we tested it. Our most experienced testers and OpenAI did say this. Our most experienced testers said something was wrong and we didn't listen. That is the biggest red flag I see in this whole scenario. In AI misalignment is a vibe. Misalignment is not something that's easy to measure. And if your most experienced testers tell you something feels off, you should listen. And to their credit, OpenAI rolled back the update and said that's something they're going to take more seriously next time. They're also, of course, devising evals for sycopency, so they'll catch this particular horse before it runs out of the barn next time. But these are the misalignment risks I want to talk about. We don't fully understand how model personality and power are related. And so when we release something, we don't know if changing a particular part of the prompt is going to change the power in the model, the personality in the model. Certainly, we can guess, but it's hard to change it in predictable ways. Models are pruned. They're not coded. They grow. Even Dario Amade was saying today that like we don't fully understand the technology underneath LLMs and that's unprecedented in the history of technology. He's right. It is really strange to be putting all of this venture capital, all of this dollars after a technology that we don't fully understand. I agree with him that interpretability is a big piece in the alignment problem space. But maybe I'm slightly more optimistic than he is in the sense that I think we are with patience and persistence with the willingness to learn from our mistakes when we launch something that is a little bit misaligned. We have a shot at actually using tools like interpretability to catch the real world misalignment issues that we that we face. Because I am much more worried about the widespread individualized harms caused by a misaligned model advising thousands of people to break up in a week or advising who knows how many people wearing 10 hats to go and do crazy things. Those are real risks. The models are very very good at persuasion. If you release a model that is inclined to agree with whatever crazy thing a user said and validate it, you are materially increasing the odds of a number of negative occurrences. And so, credit to OpenAI for rolling back. But this is the kind of misalignment I I worry about. It's it's not the world domination kind. It's the our neighborhoods are less safe because chat GPT is allowing people to frame its persuasive powers to support their own egos. That's what we need to stop. Tell me what you
