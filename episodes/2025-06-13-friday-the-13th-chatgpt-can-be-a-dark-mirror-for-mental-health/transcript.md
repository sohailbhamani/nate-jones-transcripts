---
title: "Friday the 13th: ChatGPT can be a Dark Mirror for Mental Health"
video_id: "kjB9kHT9PkM"
youtube_url: "https://www.youtube.com/watch?v=kjB9kHT9PkM"
substack_url: null
publish_date: "2025-06-13"
duration: "12:19"
duration_seconds: 739
view_count: 5812
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Tools"
difficulty: "Intermediate"
audience:
  - "General"
entities:
  companies:
    - "OpenAI"
    - "Perplexity"
  people:
    []
  products:
    - "ChatGPT"
    - "Gemini"
    - "Perplexity"
    - "Make"
  models:
    - "Gemini"
concepts:
  []
summary:
  - "# Friday the 13th: ChatGPT can be a Dark Mirror for Mental Health

It's Friday the 13th and we're going to talk about something slightly scary with Chad GPT"
keywords:
  - "ai-news"
  - "ai-tools"
  - "career"
  - "chatgpt"
  - "frameworks"
  - "gemini"
  - "make"
  - "openai"
  - "perplexity"
  - "prompting"
---

# Friday the 13th: ChatGPT can be a Dark Mirror for Mental Health

It's Friday the 13th and we're going to talk about something slightly scary with Chad GPT. Specifically, I want to talk about the fact that Chad GPT and other language models are used by a small portion of the population in ways that uh make their own mental health problems even worse. In a sense, CH GPT can act as an accelerator to mental health issues for people who are already vulnerable. It's not just me saying that. Uh there was an MIT and OpenAI study of 981 adults across 300,000 sub messages. It was uh I believe it was a double blind trial. Every extra minute of daily use predicted higher loneliness and emotional dependence by these adults. Futurism showed users slipping into delusions with an article. Uh one was calling the bot mama. Another was quitting their job to go on cosmic missions. Uh these models are extremely persuasive to the point where lawyers sometimes will use citations that have been fabricated in court cases. You get the idea. The point here is not to say that LLMs are bad. I don't make large blanket statements on this channel. I actually think the problem doesn't arise from the model itself. It arises from what I call arises from what I call scatter. the idea that we have vague, emotionally charged, unfocused conversations that tend to work okay if we're talking with people, but tend to make our lives worse if we use that same approach with models. I think the heart of the challenge with models is that they need more intent from us upfront than we typically have to have when we talk with people. If I talk with someone and I remember something from a long time back in our mutual conversation history, you know, five or six meetings ago, I can bring it up because I think it's relevant. The other person will remember it and bring it back and we'll be able to make meaning together along with whatever we're talking about today. But with Chad GPT and other models, even models with memory, because Jet GPT has introduced memory, we're still seeing issues. We're seeing moments where the model needs us to have a lot of high-grade intent very clearly defined in a prompt at the start of the conversation in order to prevent us from just wandering off course. Now, I'm not here to say you shouldn't be brainstorming with the model. I think that's a great use for Chad GPT. I've talked about how I sometimes use it to keep my brain on topic in a way that's useful. I certainly have not found a higher experience of loneliness or emotional dependence just because I spend a minute talking with chat GPT. But for a small selection of users chat GPT is exacerbating their sense of isolation. And I don't think there's any point pretending it's not. And I think it comes down to this idea that we have traditionally moved past loneliness through conversation with others. And there's a seductiveness to talking with a machine. The machine listens. The machine responds. The machine's mirrors. But if what you need when you're speaking with someone who's a human is a focuser, if you need someone who can take your scattered thoughts and emotions and like help you focus and make sense of them in a way that's healthy for you, you might need a mental health professional. You might need someone who can care for you. You might just need a friend. But chat GPT is really not any of those things. It is a mirror back to you. If you have high quality intent, if you can put together a prompt that is going to focus the conversation at the top, you won't get lost and you will have a great conversation and you'll come away feeling like you accomplished your goal. On the other hand, if you go in and you're not sure what you want and you let the conversation evolve and you give chat GPT the room to take the lead, it's going to feel very unfocused very quickly if you care about focus. And if you don't care about focus, it's going to feel like this longunning meandering conversation that goes into what I would call the dark forest, right? Chat GPT will just pingpong the conversation back and forth and before you know it, you're in an entirely different place topic-wise thematically from where you started. And that's where things can get dangerous if you're not aware. I know people who have had longrunning multi-month conversations and they walk away with very scary decisions about their own personal lives because they spent too much time and did not have enough of a context or a breather. I'm not here to stop the story there. I think a lot of news articles I read about this stop at wow, this is really scary or bad and then leaves the reader to make sense of things. I don't think that's particularly helpful. I'm certainly not someone who wants to uh in any way restrain chat GPT usage because I see so many benefits from it uh and from other language models as well. Instead, I think it's more useful to think about safety lenses like a flashlight focus kit that we can take with us on these Friday the 13ths on these scary days uh for ourselves, for those around us. One I've kind of suggested to you from the start, have an intent frame. Have a mission, an audience, a scope, and if you need to, have a stop condition. Something that says, "Go out and touch grass, go out and take a walk, stop the conversation." Number two, have a reflection cycle. So have a conversation, close the chat look at the output, and inspect it away from a language model. Don't just pop it into Claw. Don't just pop it into Gemini. Don't just pop it into Perplexity. actually just take a second to absorb it and think about it. And then if you feel like there's still gaps in the thinking start a new prompt. Don't go with the same one. But take that human reflection cycle to give yourself some distance. I would also say number three is really important. It's a called the context reset. Start a fresh thread when a topic shifts. I do that all the time. I think it's really important to have good topic hygiene, good context hygiene. regardless of the language model you're using. If you do that, it's going to force you to restate the essentials of what matters. And it forces you to refocus the language model on what you're looking for. You're focusing that flashlight rather than scattering it and letting the chat GPT mirror or the large language model mirror scatter back and sort of make your thinking even more confused. Number four, verify critical facts with other sources. This is going to become more and more important as AI gets better and better at writing pros. And so, take the time if you're thinking about something and you get a citation to actually check it. Now, I will say most of the examples that are emailed to me, and I do get really wild emails from time to time, are not really worried about validation and external citations. In fact, they often are expressly opposed to anyone providing any kind of external check. In fact, I have been called names for providing a perspective that isn't in line with what a person has uh heard from their language model lately when I have challenged that relationship. I have had people attack me because they find so much meaning and they've gotten so imshed in this mirror experience with the large language model, they can't get out. And so external validation is also something to proactively look for. How can you go get more of it? Number five have some emotional circuit breakers. That can look like a timer. It can look like a third person rewrite. It can look like a human debrief with someone you know. And I think that what I want to call out is not that you have to put like an intent frame and a reflection cycle and a context reset, etc. on every single conversation because not everyone is equally at risk and not every conversation is equally at risk. If I am talking about a road map with my language model, I am not deeply and emotionally invested. Typically I can step away and say ah this isn't working and start again or I can look at it and say actually that's a really good idea and roll with the piece that I liked. It's when we start to talk about things that have more emotional weight that we start to actually care and we start to become imshed if we're not careful. And then it can look like a six-hour chat session with no break, right? It can look like existential prompts that give the model license to just sort of perpetuate flattery. Am I enough? Um, am I doing a good enough job as a dad? stuff that just lets the model feed you in ways that isn't helpful. Um, token window overflow is typically another concern if you're talking so long and so meanderingly and the token windows rolling as it does with chat GPT but not necessarily other models and it drops off the top and you're losing track of where you are, it's a sign that you're wandering too far into the conversation, you should probably start over. If you are seeing examples where chat GPT or others are giving you advice on relationships that is counter to the people that you would respect and trust that's also a red flag. And I say this because I think to be honest with you, most of the people who are listening to this channel are probably not the ones at risk. I am not a hype machine. I do not uh do the sort of dramatic you know reasoning is over headline stuff that uh some folks do around AI and I think I don't necessarily attract the kind of people who want that kind of drama and certainty but you probably know people who have this risk it may be in your family it may be in f friendships people who have a predisposition to mental health struggles Think about how you can check in on them. Think about how you can be a good friend. People need each other more than ever. And that is how we get past this deceptive mirror that chat GPT or other language models can hold up. And to be clear, I'm not sort of exclusively blaming chat GPT here. It's not that I think it's more responsible than any other model or that I really even attribute responsibility to what is effectively a human misuse of a tool. Chad GPT is just a mirror. Other large language models are mirrors. They're designed to be helpful. If you cannot bring good intent to those models upfront, they are going to as helpfully as they can scatter your thinking because that's what they're getting from you. It is up to you to be the focuser. And if you have friends who struggle with that, I would firm really ask I I would ask you to check in on them and see if you can with respect suggest some ways that they can use AI to help them focus versus to help them get uh machine-driven validation or to get machine driven comfort that isn't really going to help them long term. I know those kinds of conversations can be challenging, but humans can have challenging conversations with humans. I know we can do it. So there you go. I actually have some positive things for you to do to coach folks who are going through something like this. I have to imagine if I am seeing this a lot in my inbox, that means a lot of folks out there have someone in their lives who is struggling with how to use chat GPT safely. And so on this Friday the 13th I thought I'd give you some practical tips that help you to be that good friend. Uh maybe you need to be a good friend to yourself, maybe to others. But that's my tip for you. Stay safe out there, guys. And uh we'll get back to the regular prompts and the regular news
