---
title: "The Evolution of LLMs: How AI Applications Are Silently Improving"
video_id: "DdRuyU_Tj5g"
youtube_url: "https://www.youtube.com/watch?v=DdRuyU_Tj5g"
substack_url: null
publish_date: "2024-09-04"
duration: "4:35"
view_count: 383
yt_tags:
  []


# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Founders"
entities:
  companies:
    - "Cursor"
  people:
    []
  products:
    - "Claude"
    - "Cursor"
  models:
    - "Llama"
concepts:
  []
summary:
  []
keywords:
  - "ai-news"
  - "ai-tools"
  - "anthropic"
  - "claude"
  - "coding"
  - "cursor"
  - "frameworks"
  - "leadership"
  - "meta"
  - "startups"
---

# The Evolution of LLMs: How AI Applications Are Silently Improving

so you know how when you buy a car you care about the engine while some of us do well when you build an application you care about the large language model inside it if it's an AI application in these days the cool thing is that llms continue to get better under the hood and so your application can improve quietly in the background one of the reasons why cursor has taken off is because llms underneath have gotten substantially better in the last few months in some really interesting ways I want to call out a few highlights number one is file search from open AI do you recall like a year plus ago when there was a scandal because Air Canada had an llm well it was actually an NLP model that's a different detail but they had a chatbot hallucinate a bereavement policy that didn't exist and it resulted in a lawsuit that kind of hallucination would not be practical to do today if you set up your llm correctly and open ai's recent file search update makes that even harder so file search adds the ability to bring in documents from outside into embeddings which are sort of vectorized representations of the real world and then you can use both vector and keyword search to go and retrieve relevant content from that vectorized embedding to answer user queries it's a fancy way of saying if you have for instance an HR policy on something you can add it in directly into open AI super easy it will vectorize it it will come back and you can get responses from it now people have been doing this for a while with a larger tool chain approach with llama index for example but this makes it super easy to do it all with an open ai's ecosystem which is of course what they want I also want to call out that Claude this is switching topics a bit right we talked about open AI they've got their file update another one that came by is Claude Claude is increasing the token count in their API calls to 8,192 they're doubling it so it's over 8,000 tokens versus about 4,000 tokens previously this may not seem like a ton and Claude like most large language models doesn't like to use that token space if it can avoid it so you kind of have to push it but it's worth calling out that their Max token count just doubled and one of the reasons that matters is if you're doing something like writing code say and you need a larger code piece well you now have double the room and so when people talk about what llms are good at where they're weak some of this has to do with underlying capabilities that can change over time and so I think one of the things that's been interesting to observe is that the doubling of the Claud Sonet Max output token limit has has coincided with the recent rise and interest in cursor cursor has been out for a little bit but cursor has been depending on the capabilities of llms underneath and it's almost like the llms underneath had to get good enough for people to be able to write really strong code with minimal effort or at least passable code for small front-end apps with minimal effort depending on who you ask right like there there are definitely people who are skeptical on the ability of cursor plus Claude Sonet to write a complex application I have not yet seen one so we will see but regardless it is remarkable to be able to sit down put in an outline get a working app out and I think I suspect that there's a connection between the updates in Claude over the summer and the tip up in usage in cursor and I know that there's some other stuff there's sort of when open AI Founders tweets about you like you're going to have a little bit of a spike interest but it wouldn't have lasted if people couldn't build stuff with it and that underlying capability is driven by the llm and so the next time you think about what you can build I want you to think about what you are building in the direction of increasing llm capability over time what's going to work better in six months or in a year because the large language models underneath are going to get even better there you go that's my thought what are you doing with file generation what are you doing with the extra CLA uh tokens
