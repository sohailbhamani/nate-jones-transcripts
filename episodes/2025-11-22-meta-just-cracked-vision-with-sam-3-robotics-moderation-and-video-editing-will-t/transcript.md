---
title: "Meta Just Cracked Vision with SAM 3: Robotics, Moderation, and Video Editing Will Transform"
video_id: "_82WB5N7gd8"
youtube_url: "https://www.youtube.com/watch?v=_82WB5N7gd8"
substack_url: null
publish_date: "2025-11-22"
duration: "11:35"
duration_seconds: 695
view_count: 27778
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  - "AI strategy"
  - "AI agents"
  - "AI automation"
  - "large language models"
  - "AI labor market"
  - "future of work"
  - "prompt engineering"
  - "GPT-5"
  - "Gemini 3"
  - "SAM3"
  - "Nano Banana Pro"
  - "antigravity IDE"
  - "scientific reasoning models"
  - "3D generative tools"
  - "developer workflows"
  - "agentic IDE"
  - "AI jobs for teams"
  - "AI strategy for operators"



# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "OpenAI"
    - "Anthropic"
    - "Google"
    - "Meta"
    - "Cursor"
    - "Figma"
  people:
    []
  products:
    - "Gemini"
    - "Cursor"
    - "Make"
    - "Artifacts"
    - "Nano Banana"
  models:
    - "Gemini"
    - "Gemini 3"
    - "SAM"
    - "SAM 3"
concepts:
  - "Nano banana pro turns an image into an interface"
  - "Solving semantic perception"
summary:
  - "# Meta Just Cracked Vision with SAM 3: Robotics, Moderation, and Video Editing Will Transform

This was one of the biggest weeks in AI that I can remember"
keywords:
  - "ai-agents"
  - "ai-news"
  - "ai-strategy"
  - "ai-tools"
  - "anthropic"
  - "artifacts"
  - "coding"
  - "cursor"
  - "figma"
  - "frameworks"
  - "gemini"
  - "google"
  - "make"
  - "meta"
  - "nano-banana"
  - "openai"
  - "product-management"
  - "workflows"
---

# Meta Just Cracked Vision with SAM 3: Robotics, Moderation, and Video Editing Will Transform

This was one of the biggest weeks in AI that I can remember. Here's the top six stories that mattered. Number one, the release of Gemini 3. And I'm going to throw anti-gravity in there as a bonus. Gemini 3 is Google's new model. It topped most of the benchmarks, but that's not what matters. What matters is that people around the world picked up that model, started to use it, and agreed. Unlike the launch of Chad GPT5 where there was widespread disagreement and controversy around the launch itself regardless of the benchmarks, everyone pretty much agreed that Gemini 3 is a very strong model. That's certainly been my experience. I wrote up a whole post on it. Anti-gravity goes with Gemini 3. It is a fork of VS Code for developers where AI agents have full execution privileges. They can read and edit files. They can run on your terminal. They can install dependencies. You control the level of autonomy they have. They can record artifacts, plans, diffs, decisions as they go, so you can monitor and control what they're doing. Basically, anti-gravity turns VS Code into a place where agents do work. Now, this matters because Google is trying to own the developer environment, not just the model. So, if anti-gravity becomes the place where more developers write code, Google doesn't just win model usage here, they win the entire developer life cycle. And so the competitive game shifts from whose model has the highest eval score to whose environment is the default place where work gets done and where agents do real work. Google is betting that the Agentic IDE is going to become the AI operating systems shell. And so we will see how this plays out. There are obviously other players in the mix. Cursor is a big one. But Google has put their stake in the ground and said they're not just a modelmaker at this point. They want to own the development environment as well. So anti-gravity could become the central surface where agentic workflows run and the place where ultimately you shape the code that drives the compute experience. That is not a guarantee. Developers tend to be loyal to their editor. They tend to care about the ergonomics of what they're doing and they don't like to switch. And so Google is making a long-term play here and we'll have to see how it plays out. Story number two, Nano Banana Pro. It is not just an image model. It is a visual reasoning model that has solved correct text rendering and conceptual relationships. This is not about cutesy captions. It's not about special illustrations. It is about UI level image generation that can correctly do headings, labels, menu structures, multilingual content, paragraphs. It can summarize an entire earning statement into a single slide. It also supports 4K output. It can combine up to 14 images at once. Fundamentally, Nano Banana Pro turns an image into an interface. This is the first moment when image generation is now part of your regular product engineering workflow. And not just marketing, not just art. It's the image becomes a way to iterate on visual surfaces in seconds. It enables agents to plug in and iterate on visual surfaces that they couldn't run, iterate, see, build on before. So agents can build landing pages, critique them, pull them back, try new email designs, try new onboarding flows. It's as if Figma automation and slide deck automation and UI design automation and Tableau all got rolled into one. This is so new that we're still figuring out the impact, but a few places this could go. Closed loop design becomes real, so agents can generate, read text, revise, and test right in the browser. You can have entire product surfaces become codifiable. the UI just becomes another completion target. So you can generate as you go. This has always been the hype is that AI can generate as it goes. But having a visual tool like this helps to make that more plausible even if that's not the most common user interface. This is absolutely going to pressure OpenAI and Anthropic to advance their multimodal pipelines. Now one thing to watch, enterprise trust is still low for generative images. The fact that it's good doesn't mean that enterprises will immediately trust it. even if it is good enough for most enterprise use cases. Text accuracy is excellent, but layout consistency across multiple generated screens is still a hurdle. And there is a limit to the amount of text you can reasonably fit in an image, which I would argue is driven mostly by RARI's ability to process, less so by the model. But if you're trying to do very heavy text and image, this is still not the right model for that. That being said, for all practical purposes, the way to think about Nano Banana Pro is that visual reasoning has been solved and we have other problems to work on. We have to work it out how it gets into our workflows, etc. But the ability of the model to generate what we ask for and develop useful work artifacts taken care of. Story number three is SAM or the segment anything model version three. It's a computer vision model from Meta that segments and identifies concepts, not just shape. That is absolutely massive. It is a chat GPT moment for video, for three-dimensional planning, for workflows and automation and manufacturing, etc. Let me explain why. You can ask SAM 3, find every forklift in these videos. Find people not wearing safety vests in these videos. Segment every red object in this video. track the brown dog across the scene. No manual clicks, no bounding boxes, just plain language. So, SAM 3 shifts vision from like pixel geometry and finding where the shape is to semantic perception. In other words, the model can see like we do and the model becomes queriable. So, like just as you can ask a human, where is the blue trash can in this video? You can now ask the model that it turns every image, every video, every camera feed into a searchable data set. Vision becomes a natural language interface. There's a lot of implications for this. I think we're just barely scratching the surface. Annotation for AI training is going to drop from weeks to minutes. Robotics perception pipelines are going to get way simpler. Video editing is going to transform. Masking took days before and now it takes seconds. Content moderation at scale is very easy. Photo and video apps may adopt SAM 3 as a magic wand concept editor. Now, it's not perfect. Zero shot semantics are good. Concept edges can blur a little bit. It's going to get better. But just as we regard Nano Banana Pro 3 as solving visual reasoning, we should regard SAM 3 as fundamentally solving semantic perception. It is good enough. It works. Huge pressure on Google to improve their model and on Open AI by the way after this. Meta did a great job shipping this. Number four is marble world layer. I think this got slept on and I'm excited. It is a generative 3D tool that builds stable, editable, and exportable environments with Gaussian splats, polygonal meshes, realistic textures, spatially consistent rooms and buildings, and it has a chisel editor that lets you define structure and an AI that fills in details. It's from World Labs, led by famous AI researcher Fe Lee, and it matters because it makes 3D content creation workflow grade for the first time. So, this is not like a production pipe. This is a true production pipeline. It's not a research demo. And I've used it. It's incredible. 3D worlds are not just generative toys. You can actually do game development in this tool, film VFX in this tool, simulation and robotics in this tool. Essentially, spatial AI is jumping into the mainstream. This could dramatically lower the cost of previs for films. It could enable world building uh for AR and VR apps. It's almost trivial. It's an early concept or an early version of a 3D Figma, but it's actually a production application. Now, is the Fidelity absolutely perfect? No. Is it good enough that we can start to see where the future is going as far as 3D spatial rendering in AI? Yes. And that's a huge deal. Number five, G story number five, GPT5 scientific reasoning paper. This is a peer-reviewed preprint showing that GPT5 is doing real scientific works. It proves new theorems. It discovered symmetry generators in cur black hole physics. It proposed biological experiments that matched unpublished lab results. So, it couldn't have seen it beforehand. It surfaced cross-domain literature insights. The key contention of the paper is that when you look across all of these at once, it's not helping. It's actually contributing original results. This is from OpenAI, but it's not just an OpenAI internal paper. And so, there's less concern around bias there. It has academic collaborators out of Oxford, Cambridge Harvard Vanderbilt and Jackson Lab. And so why does it matter? This is the cleanest proof yet that frontier models are starting to behave like research collaborators, not just assistants. It also sort of punctures the idea that the all models are commodities now argument. For frontier reasoning, for deep math, for physics, for biology, model quality is not interchangeable. And every researcher that I have spoken to or who has spoken publicly insists that for that kind of research, GPT5 or 5.1 Pro is the gold standard. Now, there are some specialized models from Google that are good for particular applications, but if you're doing scientific reasoning as a practice, the gold standard appears to be 5.1 Pro from Chat GPT. And this continues to go with the theme that these models are specializing and the way we use them is specializing. Now, GPT5 Pro is not perfect, but again, just as we look at the 3D world generator with Marble, this is enough to show a fundamental change in role. And so, instead of thinking of chat bots as minions that go do jobs, these scientists are increasingly regarding GPT5 Pro as a thinking partner that helps them to make novel discoveries and that is able to propose and prove novel theorems that they can then validate. And that's a big step for an LLM. Story number six, OpenAI and Fox Con have created a partnership that will build a US manufactured data center optimized for AI. That includes racks, cooling systems, power delivery enclosures. It's a it's a move that signals that Frontier Labs are entering the era of physical vertical integration. And so owning the metal is going to let OpenAI deploy models faster, reduce compute bottlenecks, control costs, potentially avoid geopolitical risk, build custom racks optimized for their training stack. What's interesting is that this gives OpenAI a lot of flexibility. They can build custom racks tailored for training, for inference, for memory architecture. They can build very power efficient layouts. They can optimize data centers. This is the beginning of a hyperscaler era for physical AI factories, and I expect to see more of this. Last but not least, I wanted to show you a really cool way to visualize this week's news. So, this is a slide deck that I created using Notebook LM and I was able to get the entire news story into the slide deck. I'll share this along uh with the prompt that I used as part of my newsletter this week. And the other thing I want to call out is I actually used a specialized prompt tool to build this so that I got the full story. I basically put the full story of the narrative in and I got the prompt tool to give me a very structured prompt to make a deck like this. And I'll be talking a little bit more about that in my story on Monday. But I think this is really cool. And so I'll be sharing this deck and uh sharing the prompt that I got to build it as well. Happy Saturday. Catch your breath. And I cannot wait for what next week holds. The AI race just continues to accelerate.
