---
title: "NVIDIA told us exactly where AI is going — and almost everyone heard it wrong"
video_id: "5Kp-Gj5qXL0"
youtube_url: "https://www.youtube.com/watch?v=5Kp-Gj5qXL0"
publish_date: "2026-01-08"
duration: "18:37"
duration_seconds: 1117
view_count: 67855
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  - "AI infrastructure"
  - "CES 2026"
  - "NVIDIA Rubin"
  - "OpenAI"
  - "AI inference"
  - "LLMs"
  - "AI chips"
  - "AI factory"
  - "enterprise AI"
  - "AI scaling"
  - "GPU demand"
  - "AI hardware"
  - "inference economics"
  - "AMD AI"
  - "Anthropic"
  - "AI strategy"
  - "physical AI"
  - "AI supply chain"


# AI-enriched metadata
content_type: "Framework"
primary_topic: "AI Strategy"
difficulty: "Advanced"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "OpenAI"
    - "Anthropic"
    - "Google"
    - "Microsoft"
    - "Nvidia"
    - "AWS"
    - "Azure"
    - "AMD"
  people:
    []
  products:
    - "Gemini"
  models:
    - "Gemini"
    - "SAM"
concepts:
  - "When we talk about the ai factory future, everyone else is talking about this after openai wrote the deals to secure their own future for the next few years"
  - "There are broader shortages across the ai economy because we are so demand constrained"
summary:
  - "# NVIDIA told us exactly where AI is going — and almost everyone heard it wrong

CES is usually treated as a consumer electronic spectacle, but every so often it becomes something more than that"
keywords:
  - "ai-news"
  - "ai-strategy"
  - "amd"
  - "anthropic"
  - "aws"
  - "azure"
  - "coding"
  - "frameworks"
  - "gemini"
  - "google"
  - "leadership"
  - "microsoft"
  - "nvidia"
  - "openai"
  - "product-management"
---

# NVIDIA told us exactly where AI is going — and almost everyone heard it wrong

CES is usually treated as a consumer electronic spectacle, but every so often it becomes something more than that. It becomes the coordination event for the next industrial cycle. And that is what is happening this year at CES 2026. It's landing at the exact moment when OEM budgets, data center build plans, and partner road mapaps are all getting locked together for the year. And everybody knows this is a year for scale. So, it's where companies are not just showing gadgets this year. They're trying to signal what the supply chain should be optimized for in order to enable that scale. And it's clear from the first few days of CES that the supply chain is now being optimized for exactly one thing. Always on AI delivered cheaply and reliably at scale. Nvidia's own framing is unusually explicit about this. They say AI has entered an industrial phase. If it's industrial, you think power, you think scale, you think electricity, you think big machines. And that is exactly the right mental model for CES this week. The announcements that matter most are really not about new devices. They're the pieces of an AI factory. Compute, memory networking security power deployment velocity, because that's what determines who gets to ship intelligence at scale. So CES is where that industrial posture is becoming visible in product form. Let's go through what a few of those look like. First, we are going through a demand shock right now. I know it has become popular to ask questions about the AI bubble, but we have got to admit looking at CES that everyone who sees daily data on AI is acting like they are way behind on demand, not way overhung on demand. Inference is now the cost center that sets the architecture of the future because inference is how we serve the models at scale and we are short on demand. The foundational fact that everybody is working on is massive usage. So when Sam Alman says Chad GPT hit 800 million weekly active users back in October, it's more now we are under a permanent serving load that dwarfs the cost of any single training run for AI. And that really flips the industry's optimization target, right? Training can still matter strategically because it creates new capabilities and because any individual training run is quite sizable, but inference is dominating the operational reality of chipsets. It's continuous. It is latencybound. It's ruthlessly cost-sensitive. And so the system question is becoming, how do you drive dollars per token down while keeping latency and reliability inside SLAs's? That's the core plotline you need to be following behind the chipmaker moves that we're tracking at CES. And so CES 2026's real headline is that Nvidia is now selling an AI factory, not just GPU generation. And so the most important story is not the launch of Reuben per se or that Reuben exists. That's the new Nvidia chipset. It's actually how Nvidia is positioning their new chipsets within the larger AI ecosystem. Reuben is a rack scale platform. It's not just an individual chip and it's designed around token economics first because it's intended to be serving inference loads. So Nvidia launched the Reuben platform at CES as a sixchip extremely designed system with the Vera CPU, the Reuben GPU, NVLink 6 switch, Connect X9, Super NIC. I can say other things and you probably believe me because you just say acronyms, but but take my word for it. Essentially, Nvidia launched a rack scale chipset that slashes inference token generation cost by a factor of 10 while serving inference loads much much more quickly. And so what they are optimizing for is extremely efficient service of very large models and very large context windows. The Reuben chip is going to be able to handle 10 million token context windows. The Reuben chipset is going to be able to handle serving models like 5.2 Pro much much faster than we can serve them today. And how do we know this? Because Nvidia productized inference context memory with the Reuben chipset. So the most inference era move is Nvidia's inference context memory storage platform. It's it's an attempt to push the key value cache and context out of the GPU itself and into a storage tier so you can reuse it instead of recomputing against it all the time. And Nvidia's developer writeup frames this directly as KV cache growth becoming a scaling constraint as you serve larger and larger context windows. And for those who don't know, KV cache is something that you have to compute against when you are serving next token generation because the KV cache absorbs the tokens that you have been given as part of the context window. This is a big conceptual shift. context has become a managed resource at this point just like a cache or a database tier is managed in a classic web stack. So when the platform vendor makes that a product, it's basically an admission that inference scaling is now a memory and data movement problem as much as a compute problem. And we see that in the way they've designed the rack scale chipset, the Envy Link 6 switch, the Connect X9, those aren't just random acronyms. It turns out those are part of the firmware and hardware advantage Nvidia brings to the table because they're designed to provide interconnects and data throughput that allows the entire rack scale chipset to work and serve data at scale and really to work and serve much larger context loads at scale. If you want the most complete endto-end story of what it's going to look like to have Reuben out in the wild, treat OpenAI as a reference customer for the AI factory era. The 2025 infra infrastructure deals that OpenAI wrote describe the entire stack and have implications today. Compute supply is being industrialized in parallel and OpenAI is leading the way on. So Nvidia, we were just talking about them. OpenAI and Nvidia announced a letter of intent to deploy at least 10 gigawatts of Nvidia systems. The first gigawatt is planned for the second half of 2026 on those Vera Rubin chips with Nvidia intending to invest up to a hundred billion progressively as deployment scale. Notice it's framed in power. Notice that it's 10 gawatt. Like we are now thinking in terms of dollars per token at the headline level. This is industrial AI lane two in parallel. They are not done with just an Nvidia partnership. OpenAI and AMD announced a partnership for another 6 gawatt of AMD deployment. And AMD obviously issued OpenAI a warrant for 160 million shares tied to deployment milestones. And they're also looking to bring the first gigawatt online in the second half of this year in 2026. Again, they're measuring in power. It's smaller than Nvidia. It underscores how much demand OpenAI sees up ahead for their models. They can't get enough out of Nvidia even though Nvidia is the premier provider. And we're still not done because OpenAI and Broadcom announced a collaboration to deploy another 10 gigawatts of OpenAI designed AI accelerators and rack systems. And that's targeted to start in the second half of 2026 and be complete by the end of 2029. And so OpenAI is not necessarily choosing a chip here. It's essentially saying we are so big and demand is so high. We are building a portfolio of supply because the constraint is not theoretical compute. The constraint is delivered compute to our users at scale. Sam Alman has been very open about the fact that he cannot find trillion tokensized packages to deliver to enterprises and he needs them and he's going to need 10 trillion. He's going to need 100 trillion token packages to deliver to enterprises because individual developers in 2025 last year were doing 10 billion tokens at times. And so if one developer can do 10 billion, it adds up into the trillions awfully fast. But we're still not done with the OpenAI as a factory story. Capacity is also being bought as cloud contracts. So OpenAI's $ 38 billion AWS deal is best read as a capacity lock while longerterm AI factories are gradually coming online. They're securing the cloud now so they can continue to serve while they build out this larger agreement to scale Nvidia GPU capacity, to scale AMD, etc. And we're still not done because they also carved out a multi-billion dollar deal with Coreweave to build AI capacity that Coreweave can help them serve for inference as well. Fundamentally, when we talk about the AI factory future, everyone else is talking about this after OpenAI wrote the deals to secure their own future for the next few years. That was a really smart move on OpenAI's part. It's going to guarantee them a place at the table for 2026 and 2027. no matter what happens and it will give them room to keep growing, scaling and serving their models with fewer bottlenecks than players who came late to the table. Now, this is the piece that most people tend to miss. Open AAI also announced that Samsung and SKH Highix are joining the SCAR Stargate project with a target of 900,000 DRAM wafers per month, which is critical to enabling the memory that drives OpenAI's model. NVIDIA's CES move essentially formalizes inference context memory storage and it's part of a platform level mirror to what OpenAI is doing by securing the SK highix contract. And so what I want you to understand is that when you look at the headlines like what Nvidia announced with the Vera Rubin chipset, what you should see is a simplified packaged up picture of the AI factory of the future. And we can see all the pieces of that factory coming together in individual deals that OpenAI put together over the last year. The thing is, everyone else is going to need Nvidia's chipsets because the broader market is already reflecting the scarcity created by OpenAI's deals. Reuters is reporting that DRAM prices are now up over 300% in Q4 as AI demand has tightened conventional supplies. Reuters also cites counterpoint which calls out that in the third quarter of 2025 bandwidth memory share was incredibly dominated by just two players Samsung and SK highix the very players open AI cut a deal with the HBM race is like so much of the rest of the AI race turning into a bottlenecked competition for the supplies of just a few vendors. You see this with Zeiss, you see this with Nvidia and their GPUs, with SKH Highex, with ASML. Fundamentally, there are broader shortages across the AI economy because we are so demand constrained. There is no shortage of AI demand. And again, I'm going to call out that is not what typical bubbles look like. I was there in 2000 and uh the problem was not that we had too much traffic on the internet at that time, but that's the problem now. So, of course, the question to ask after the VR Rubin launch is, are there signs that anyone is really threatening Nvidia's dominance? If threat means Nvidia gets displaced as the default platform in the next year to 18 months, the overwhelming evidence says no. OpenAI's own 10 gawatt Nvidia commitment is a great example. But I also want to call out that if threat means Nvidia's share of inference spread spend might decline over time, then potentially yes. And the mechanism is really structural and demand driven. There are three real pressures on Nvidia right now. First, second source hypers scale GPUs like AMD. So, Google's AMD deal is precisely the kind of anchor customer commitment that makes a second ecosystem viable at scale. And OpenAI is really doing that because their demand is so great, they need a second partner. This in turn can catalyze the existence of a second alternative ecosystem alongside Nvidia. This reminds me a lot of how AWS was initially the only game in town for cloud and over time as customer needs grew, Microsoft had Azure, we had the Google Cloud Platform start to spin up and now you have situations where some customers are multicloud, some customers are picking one of those individual clouds. But regardless, each individual player in that system, including AWS, has been able to continue to grow even as their relative share of the total cloud spend has changed over time. The other pressure on Nvidia is custom accelerators for predictably serving workloads, which is what the Broadcom deal with OpenAI is all about. You don't take on custom silicon, which is what the Broadcom deal does, unless you expect inference volume to be so high that it justifies specialization at the chip level at scale. And so that's a case where essentially OpenAI is saying our demand volume is so large, we need specialized chips. We can't do it any other way. And finally, hyperscalers are exporting their in-house chips. So, Anthropic's TPU expansion from Google is a really clean signal that large labs will train and serve on non- Nvidia silicone when the price for performance and the availability are compelling enough. And the prediction is that Anthropic will have over a gawatt of TPU capacity online in 2026. I still do not expect Google to let too many TPUs out of the house because the TPU advantage in the house for Gemini is so compelling as a competitive advantage. That being said, Google's an investor in Anthropic. It's definitely to their advantage to invest in anthropics models and quality and I think it makes sense for them to put the TPUs there. And so I think we're looking at a future that has it's a mini winners's reality, right? And this is the nuance that a lot of summaries miss. demand is so high that Nvidia will remain the biggest absolute winner and I still see meaningful scale for second players in the industry because demand is exploding so fast and so competition looks less like dethronement from another player in this case and more like a multi-ecosystem coexistence a multi cloud environment if you will especially in inference where heterogeneity is very operationally tolerable and cost pressure is extreme you can train on the same chipsets and is much much more preferable to ensure a clean training run, but inference is a little bit looser and more flexible and you can serve models from multiple different chipsets if you configure them. Now CES always has some fun stuff that comes with it and ironically that's part of the same industrial story writ large and so CES is making this larger shift le legible because it shows that AI is escaping the data center and appearing as physical AI as ambient intelligence and that's going to continue to increase inference demand and diversify where AI is running. The story continues to be demand is running ahead of compute. In fact, Nvidia's CES presentation explicitly linked Ruben to open models for robotics and for autonomous operations and showcased AI defined driving in the Mercedes CLA demo as part of a broader platform story of where AI is going to show up. And this matters because physical AI is very latency bound. It's very reliability bound and increases increases the value of inference optimization which is exactly where Reuben is aiming. And really the larger story I'm telling here is it's what Open AI is doing as well. Consumerf facing embodiment like the Lego smart brick are probably how the public notices some of this shift. But the real driver I hope you're getting is industrial infrastructure. It's AI becoming a fullyfledged industry, perhaps the most valuable and largest industry on the So, summing it all up, CES 2026 is the moment the industry stopped pretending that we were in a chip race and started acting like we were in a factory race where inference economics, memory, supply, and power constraints are actually determining who gets to ship intelligence. And I took time to tell the Open AI story because I think it fits right in here. OpenAI wrote the contracts in late 2025 that enable them to be competitive in this factory race. They saw it and Nvidia has responded by selling Reuben as a rack scale AI factory and productizing the context management systems, the KV cache management systems that enable inference with less constraints on context management and data movement. So compute is no longer your only constraint. You also have to think at rack scale. How is memory being managed at the rack scale? allows data moving at the rack scale at a pace that allows us to serve inference loads for highquality models in real time and that's where all the deals on memory fit in right all the DRAM deals the conversations with SKH highex the focus on making sure that we can accelerate custom chipsets with broad comet openai those are all deals that enable a sophisticated serving of context windows in inference compute at runtime as I ask for it and so if If you want to think about what is the world that's being enabled here, it's a world of ambient AI everywhere. It's a world of very sophisticated AI everywhere. So, we're talking about the Mercedes CLA. We're talking about autonomous robots in the factory. We're also talking about GPT 5.2 Pro quality models being served almost instantly because tools like Vera Rubin are enabling extremely rapid inference at token efficient unit economic prices. And so, when you look at that story overall, it is a story that is too big for any one company. I know that OpenAI has carved these deals out and has secured a place for the future, but you can see through the TPU deals that Anthropic has rung out and you can see through Nvidia's move that there are many winners here and Nvidia is question unquestionably showing through the VR Rubin chipset that they are going to remain the biggest player in the chipset conversation for AI for the foreseeable future. and inference is so big that they will not be the only game in town. And OpenAI through the deals they've built have ensured that there will be a multi-winner hardware landscape where second sources and custom silicon can take on meaningful share without anyone really losing. The pie is going to keep growing so rapidly, everyone is going to get more and more business. And so this was the year that AI became a factory and that is going to enable a layer of industrial scale for AI applications that is going to change the way all of us work. Instead of AI being locked away in the specialized corner of our computers in the chat window, AI is going to be on every single surface of our digital experience and also moving very rapidly into robotics. And to do that, you have to have the factory scale. And that's that's what we're seeing at CES this year. This this is the future getting unlocked right now. So let me know what you think is the most important development of CES and uh I'll see you in the comments.
