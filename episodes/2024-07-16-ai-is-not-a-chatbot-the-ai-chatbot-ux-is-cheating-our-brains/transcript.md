---
title: "AI is not a chatbot:  the AI chatbot UX is cheating our brains"
video_id: "ODLyBQd4VHU"
youtube_url: "https://www.youtube.com/watch?v=ODLyBQd4VHU"
substack_url: null
publish_date: "2024-07-16"
duration: "17:28"
duration_seconds: 1048
view_count: 3032
author: "AI News & Strategy Daily | Nate B Jones"

yt_tags:
  []



# AI-enriched metadata
content_type: "Opinion"
primary_topic: "AI Strategy"
difficulty: "Intermediate"
audience:
  - "Engineers"
  - "Executives"
  - "Product Managers"
entities:
  companies:
    - "Anthropic"
    - "Apple"
    - "Box"
    - "Twitter"
  people:
    []
  products:
    - "Make"
    - "Canvas"
  models:
    []
concepts:
  []
summary:
  - "# AI is not a chatbot:  the AI chatbot UX is cheating our brains

AI is not a chat box it's just not and I want to talk about that because I think that we have overstated the value of the chatbot beca"
keywords:
  - "ai-agents"
  - "ai-news"
  - "anthropic"
  - "apple"
  - "box"
  - "canvas"
  - "frameworks"
  - "make"
  - "product-management"
  - "twitter"
---

# AI is not a chatbot:  the AI chatbot UX is cheating our brains

AI is not a chat box it's just not and I want to talk about that because I think that we have overstated the value of the chatbot because we have seen how powerful the release of chat GPT 3.5 was in the world landscape and that specific release changed our perception of what generative AI is capable of that's great but one product release does not make a user interface and so I actually want to talk through where I think the capacities of llms are going and then I want to revisit and say how does this not fit a chatbot interface because I want you to walk away from this with some more open Horizons around where we could be designing uh llms into our applications we need more designers thinking about how llms could be more effectively deployed outside that little chat window that you see in the corner okay so the first thing I want to call out there will be five of these I want to call out on the capacities of AI by the way so number one AI should be thought of as Deployable intelligence in other words we've never really had intelligence that we can deploy management Theory would say that you can deploy intelligence among your employees but if you're working with fairly high-powered knowledge workers they are deploying their own intelligence autonomously and they should be that's great AI is directly Deployable by the person managing it and that means on the one hand that you have to put time into managing it and I think that we're going to start to see llms develop more agentic properties so they'll be able to do things autonomously more over the next couple of years which means eventually you're going to get to a point where you can delegate in a way that's sort of similar to the way you would delegate to a high-powered employee and they will just go off and and go after a particular task or particular objective independently that's not where we are today but I I think even if we get there Deployable intelligence is still the right frame because at the end of the day they are going to be accountable to the person who is driving that delegation in a way that an employee is not an employee is expected to exercise business judgment and an llm may give you business perspective but I think their ability to exercise business judgment is going to be one of the last things that AI is able to conquer so to speak and the reason for that is that business judgment is a very context dependent art and it's very very difficult to get right and I actually haven't seen a lot of evidence that llms are getting better at it despite their progress in a lot of other areas they are certainly getting better at talking business but business judgment is actually an act of decision against a particular set of data and I think part of why llm struggle with that is they can't really take in a lot of the implicit data that business judgment is grounded in they take in explicit data and so if you're looking at the complexity of the politics in a particular organization as part of your business judgment llms can get that if you exhaustively describe it to them but that's still a low Fidelity picture of what's really going on because they can't experience it that's just one example so wrapping that back to the top fundamentally if we think of AI as Deployable intelligence we will have a much more flexible view of where it can be deployed it's not just in a chatbot all right number two AI is fast and therefore cheap and I think that is one of the fundamental attributes that we have forgotten chatbots are designed to be callon response right like you're supposed to have an interaction what we miss there is this idea that llms are fundamentally much much faster at doing a bunch of tasks once they're trained than humans are and so even if they do them at lower Fidelity if we are tolerant of a lower Fidelity task done with additional guidance we you're going to go way faster on a whole range of knowledge working tasks and that's in anecdotally what I see AI being used for in organizations people are using it to do a bunch of tasks that would previously take a long time and they're just getting it done faster all right uh number three AI is opening up new business models because of that cheapness and speed and I want to call that out separately because we are just at the beginning of uncovering what that looks like I think that what we see with the uh layoffs at McKenzie is a good example of this McKenzie basically functioned on this idea that human intelligence is something that is hard to replicate anywhere else and so if you have human intelligence as a consultant you sort of have for lack of a better term a monopoly or a corner on the ability to provide consultant services and I think what McKenzie and others are discovering is that at the end of the day a lot of people are using chat GPT as their cheap MBA consultant and even if it's not quite as good as McKenzie uh depending on your opinion of McKenzie it's still good enough and that's really the fundamental issue a substitute doesn't have to be 100% equivalent it can be good enough and McKenzie is finding out that a lot of companies in an era of belt tightening when the cost of cash is higher they're going to be looking for cheaper options and there's this new technology right at the CEO's fingertips that basically functions as an MBA on top so the reason I'm calling that out is that we have gotten to a point where we're starting to see the old business models disrupted but we have not yet gotten to a point where we're seeing the new business models come into play and I think one of the interesting hints at new business models is the insistence companies have on training their own models they want to see their own models ins inside the house trained on their data that are effectively private models that they can use the way they see fit and I think that that may be a hint that we are seeing a move toward for lack of a better term vertically integrated intelligence so there's this idea in business that if you vertically integrate you gain efficiencies right you reduce costs up and down the supply chain what we're seeing here is potentially that businesses want to vertically integrate the intelligence stack and they therefore can do more with what they have if they are building inside the house they save costs but they also save on risk right like they know what that AI will produce they know what that model is capable of and I think the risk piece is bigger anecdotally than the cost savings piece right now no one wants the debacle from Air Canada where the chatbot started to issue fake policies and people see vertical integration is a way to handle that so my guess is the new business model disruption may actually look a lot like vertical integration and what's interesting is that may Empower smaller firms who are tightly vertically integrated and who have a compelling value proposition to compete really really effectively because they can move fast as a business and they're moving fast as a business because like AI is at the core of everything they do we will see it's a guess uh but I think fundamentally we're at a stage where we're sort of at that crossover point where we're starting to see the disruption of the old models like McKenzie and we're starting to just begin to see hints of the new business models okay number four smart deployment is a function of Habitual access and so this is skipping over to the UI side of things if you are building for llms you need to build assuming that you have to meet a very low friction bar for any kind of success and I see a lot of people assuming that a low friction bar means what it doesn't for lack of a better term people people assume a degree of commitment to using AI that is absurd I've seen people say oh they'll go to the special page on my Wiki and they'll use my special model they're just going to use chat GPT because that's the default in their heads like that's one of the consequences of having a highly successful external product is that people have been wired to use this UI and even if it's not an ideal UI they're still using it because that's the default habit and so if you want to deploy an llm you have to be thinking as a product person how do I reduce the friction in this space so that it is so easy to be habitual with this where are people already in my space and where can I put the llm right there where they're where they're at so they have the habit of using it in the spaces they're already in I think that's really important to drive effective usage okay fifth one generative AI means hallucination it means it because you're gener generating data and I think that when people talk about hallucination they keep talking about it like a defect and it's not it's built into the system you're designed to generate data well it's going to generate data and generating data means inherently some of the data is not going to be factual and so we need to get past the point where we assume Hallucination is a defect from a technical point of view and we look at it as an undesirable outcome from a business point of view and there's two comments I have there number one I think there's a billion dollar opportunity out there for a third party who can validate the factual accuracy of llm outputs maybe that's a service on tap where you just call them and they validate it I have no idea I'm not building it but somebody should somebody should think about the problem of factual accuracy as if it's a first class problem because it's about to be something that people are going to pay big bucks to be sure of and that is a lot of the reason why companies are putting all this effort into fine tuning and building those internal models as I touched on earlier they want to be in control of the risk imagine a world using the Air Canada example where the llm had to call a factchecking service before it could put that policy language out there in front of the customer it would be a different world we wouldn't have that story in the news because the llm would have given the correct response and so businesses May build this internally using tools like Lang chain but fundamentally I think that factual accuracy as a service is going to be an interesting opportunity and now I I grant you learning the facts inside the business is something that a third party service May struggle with so there's still going to be that internal element but we have a lot of widely accepted facts around the world that are publicly available that are in newspapers that are in scientific Publications those are all things that a third party service could act as a Data Bank for and fact check against Food For Thought I will also say the second Point here on generative Ai and hallucination I have seen this personally get better and I would bet that even though we may need a thirdparty industry or service of some sort to to validate the outputs of generative AI we are still going to be smart to bet on progress from the man model manufacturers here open Ai and others are going to get better at producing more factual AI outputs and they're not going to talk about how they did it but they're going to get better and I've seen that since starting to use chat GPT 3.5 is that it start it's getting better every generation at producing outputs that are factual and that is reducing my risk radar which is dangerous in one sense because I probably should be more thoughtful about those edge cases that still emerge than I am but as a human being I have to make risk assessments all day and I am starting to find that it is Dependable enough that I am relaxing and trusting it more and that's a very intuitive thing and maybe that's just me but my sense as those models are getting better over time as open Ai and others who are building those models start to get more and more emphatic about driving clear factual output so that they don't get in trouble anyway let's recap five things that militate against this idea of a chatbot and I'm going to kind of get into how chat Bots are ineffective in the next section but this is the first section so the five things where AI is Deployable intelligence which means it's much more widely deployed able than a chatbot AI is fast and therefore cheap which means that it doesn't have to be limited to a chatbot right AI opens up new business models that's more of the business model side of things AI is all about smart deployment as a habitual access point and so that's a UI thing where where I was talking about the fact that you have to put the llm where people are and not just assume people will go to a special page or whatever to use it and finally generative AI means hallucination and we should stop pretending it doesn't and that also has chatbot implications because chatbots and we're just getting into this second section here right the this second section of the video is all about the three flaws that I think are really killing the chatbot as a UI for llms so the first one that I want to call out uh and I just jumped into it is that chatbots simulate a human conversation and that means that they cause hum humans to overstate the veracity or the accuracy of llm outputs the problem is that humans believe chatbot outputs more because it looks like the kind of conversation we're used to having in text messages with other humans all day long and that's a big problem and I think if we had a different UI we might be tuned to believe chatbot outputs less compellingly right like we would be less likely to believe them second reason or second issue with llm uh in chatbots llm capacity continues to evolve but the chatbot interface largely is not I think anthropic has done a little bit of a push on that by S of starting to develop a canvas where the chatbot can paint something on the side like a chart and that's a great step but in general the chatbot interface is staying static as models evolve and so people are actually not really keeping track of how models are evolving because there's no indicator to think to say in the interace face that chat chat GPT 40 is smarter than chat GPT 3.5 if you are like me and you study this all day then you know that they are but if you're just a casual user using the chatbot it looks exactly the same and apple figured out that you have to make the iPhone look different car manufacturers figured out you have to make the car look a little bit different in order to sell the new car and I think there's something some insight there around human psychology in sort of how we handle interfaces with evolving models and I kind of want to put a pin in that for a future conversation okay the third reason that chat Bots are an issue chat Bots require Advanced llm knowledge because it's just a text input box it's up to you to know enough about the llm to use it well and I have literally seen sidebyside examples where I've been prompting next to someone else at the same time and I'm getting different results and it's not just that the generative AI is producing hallucinations it is that the generative AI is something I am more able to prompt because I know more about it and I know more about how it works than the person sitting next to me and that is fundamentally inequitable and I mean that in the sense that if the Deployable intelligence is there we should be developing user interface principles that allow anyone to access that Deployable intelligence it should not be dependent on the ability to read an exhaustive number of Twitter posts and read a bunch of articles about how llm works and prompting in order to drop an effective conversation with a chatbot you should be able to Max the capacity without that esoteric or hidden knowledge you just should okay so where did we end up with all this I wanted to call out sort of that those five themes that I did in the first half of the video around how AI is changing in ways that make it bigger than a chatbot and then I wanted to spend the second half talking about the three issues I see specifically with chat Bots because I think we don't talk about them enough so so I talked about how chat Bots require Advanced llm knowledge I talked about how llm capacity is evolving in ways the in for interface is not and I also talked about the fact that chatbots simulate human conversation in a way that probably causes us to overstate their intelligence and the veracity of facts that they give us I think those are all concerning so where I net out on this is I think we're in an inflection point where we need both new business models and also new UI models for large language models like I I just think we do and I know overused the word models here but it's an AI talk so what do you want um so that's where I'm netting out and I would be interested to hear examples in the comments of where you see Mo business models or user interface models that you think are a direction that the future could build on as far as how we deploy large language models in Tech because I'm always looking for new examples all right this has gone on long enough cheers
